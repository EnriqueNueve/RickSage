{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b9158f7",
   "metadata": {},
   "source": [
    "## DCASE 2020 Task 4 Metrics and Loss\n",
    "https://github.com/turpaultn/dcase20_task4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4151bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "import pprint\n",
    "import functools\n",
    "import itertools\n",
    "import typing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa89aaf",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5d8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resolve_permutation(loss_matrix):\n",
    "    \"\"\"Resolves permutation from an all-pairs loss_matrix input.\n",
    "\n",
    "    Args:\n",
    "        loss_matrix: tensor of shape [batch, source, source]\n",
    "            axis 1 refers to the estimate.\n",
    "            axis 2 refers to the reference.\n",
    "    Returns:\n",
    "    permutation: tensor of shape [batch, source, 2] such that\n",
    "        tf.gather_nd(estimates, permutation, 1) returns the permuted estimates\n",
    "        that achieves the lowest loss.\n",
    "    \"\"\"\n",
    "    batch = loss_matrix.shape[0]\n",
    "    source = loss_matrix.shape[1]\n",
    "\n",
    "    # Compute permutations as vectors of indices into flattened loss matrix.\n",
    "    # permutations will have shape [batch, source!, source, 1].\n",
    "    permutations = tf.constant(list(itertools.permutations(range(source))))\n",
    "    permutations = tf.expand_dims(tf.expand_dims(permutations, 0), 3)\n",
    "    permutations = tf.tile(permutations, [batch, 1, 1, 1])\n",
    "\n",
    "    # Expand loss dimensions for gather.\n",
    "    # loss_matrix.shape will be (batch, source!, source, source)\n",
    "    loss_matrix = tf.expand_dims(loss_matrix, 1)\n",
    "    loss_matrix = tf.tile(loss_matrix, [1, permutations.shape[1], 1, 1])\n",
    "\n",
    "    # Compute the total loss for each permutation.\n",
    "    # permuted_loss.shape will be (batch, source!)\n",
    "    permuted_loss = tf.gather_nd(loss_matrix, permutations, batch_dims=3)\n",
    "    permuted_loss = tf.math.reduce_sum(permuted_loss, axis=2)\n",
    "\n",
    "    # Get and return the permutation with the lowest total loss.\n",
    "    # loss_argmin.shape will be (batch, 1)\n",
    "    loss_argmin = tf.math.argmin(permuted_loss, axis=1)\n",
    "    loss_argmin = tf.expand_dims(loss_argmin, 1)\n",
    "\n",
    "    # permutation.shape will be (batch, source, 1)\n",
    "    permutation = tf.gather_nd(permutations, loss_argmin, batch_dims=1)\n",
    "\n",
    "    return permutation\n",
    "\n",
    "\n",
    "def _apply(loss_fn: typing.Callable[..., tf.Tensor],\n",
    "           reference: tf.Tensor,\n",
    "           estimate: tf.Tensor,\n",
    "           allow_repeated: bool,\n",
    "           enable: bool) -> typing.Any:\n",
    "    \"\"\"Return permutation invariant loss.\n",
    "\n",
    "    Note that loss_fn must in general handle an arbitrary number of sources, since\n",
    "    this function may expand in that dimention to get losses on all\n",
    "    reference-estimate pairs.\n",
    "\n",
    "    Args:\n",
    "        loss_fn: function with the following signature:\n",
    "        Args\n",
    "            reference [batch, source', ...] tensor\n",
    "            estimate [batch, source', ...] tensor\n",
    "    Returns\n",
    "        A [batch, source'] tensor of dtype=tf.float32\n",
    "        reference: [batch, source, ...] tensor.\n",
    "        estimate: [batch, source, ...] tensor.\n",
    "        allow_repeated: If true, allow the same estimate to be used to match\n",
    "          multiple references.\n",
    "        enable: If False, apply the loss function in fixed order and return its\n",
    "          value and the unpermuted estimates.\n",
    "\n",
    "    Returns:\n",
    "        loss, A [batch, source] tensor of dtype=tf.float32\n",
    "        permuted_estimate, A tensor like estimate.\n",
    "    \"\"\"\n",
    "    reference = tf.convert_to_tensor(reference)\n",
    "    estimate = tf.convert_to_tensor(estimate)\n",
    "\n",
    "    if not enable:\n",
    "        return loss_fn(reference, estimate), estimate\n",
    "\n",
    "    assert reference.shape[:2] == estimate.shape[:2]\n",
    "    batch = reference.shape[0]\n",
    "    source = reference.shape[1]\n",
    "    \n",
    "    # Replicate estimate on axis 1\n",
    "    # estimate.shape will be (batch, source * source, ...)\n",
    "    multiples = np.ones_like(estimate.shape)\n",
    "    multiples[1] = source\n",
    "    estimate_tiled = tf.tile(estimate, multiples)\n",
    "\n",
    "    # Replicate reference on new axis 2, then combine axes [1, 2].\n",
    "    # reference.shape will be (batch, source * source, ...)\n",
    "    reference_tiled = tf.expand_dims(reference, 2)\n",
    "    multiples = np.ones_like(reference_tiled.shape)\n",
    "    multiples[2] = source\n",
    "    reference_tiled = tf.tile(reference_tiled, multiples)\n",
    "    reference_tiled = tf.reshape(reference_tiled, estimate_tiled.shape)\n",
    "\n",
    "    # Compute the loss matrix.\n",
    "    # loss_matrix.shape will be (batch, source, source).\n",
    "    # Axis 1 is the estimate.  Axis 2 is the reference.\n",
    "    loss_matrix = tf.reshape(loss_fn(reference_tiled, estimate_tiled),\n",
    "                           [batch, source, source])\n",
    "\n",
    "    # Get the best permutation.\n",
    "    # permutation.shape will be (batch, source, 1)\n",
    "    if allow_repeated:\n",
    "        permutation = tf.math.argmin(loss_matrix, axis=2, output_type=tf.int32)\n",
    "        permutation = tf.expand_dims(permutation, 2)\n",
    "    else:\n",
    "        permutation = _resolve_permutation(loss_matrix)\n",
    "    assert permutation.shape == (batch, source, 1), permutation.shape\n",
    "\n",
    "    # Permute the estimates according to the best permutation.\n",
    "    estimate_permuted = tf.gather_nd(estimate, permutation, batch_dims=1)\n",
    "    loss_permuted = tf.gather_nd(loss_matrix, permutation, batch_dims=2)\n",
    "\n",
    "    return loss_permuted, estimate_permuted\n",
    "\n",
    "\n",
    "def wrap(loss_fn: typing.Callable[..., tf.Tensor],\n",
    "         allow_repeated: bool = False,\n",
    "         enable: bool = True) -> typing.Callable[..., typing.Any]:\n",
    "    \"\"\"Returns a permutation invariant version of loss_fn.\n",
    "\n",
    "    Args:\n",
    "        loss_fn: function with the following signature:\n",
    "        Args\n",
    "            reference [batch, source', ...] tensor\n",
    "            estimate [batch, source', ...] tensor\n",
    "            **args Any remaining arguments to loss_fn\n",
    "    Returns\n",
    "        A [batch, source'] tensor of dtype=tf.float32\n",
    "    allow_repeated: If true, allow the same estimate to be used to match\n",
    "      multiple references.\n",
    "    enable: If False, return a fuction that applies the loss function in fixed\n",
    "      order, returning its value and the (unpermuted) estimate.\n",
    "\n",
    "    Returns:\n",
    "        A function with same arguments as loss_fn returning loss, permuted_estimate\n",
    "    \"\"\"\n",
    "    def wrapped_loss_fn(reference, estimate, **args):\n",
    "        return _apply(functools.partial(loss_fn, **args),\n",
    "                  reference,\n",
    "                  estimate,\n",
    "                  allow_repeated,\n",
    "                  enable)\n",
    "    return wrapped_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6cb9b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_signal_to_noise_ratio_from_power(signal_power, noise_power, epsilon):\n",
    "    \"\"\"Computes the signal to noise ratio given signal_power and noise_power.\n",
    "\n",
    "    Args:\n",
    "    signal_power: A tensor of unknown shape and arbitrary rank.\n",
    "    noise_power: A tensor matching the signal tensor.\n",
    "    epsilon: An optional float for numerical stability, since silences\n",
    "        can lead to divide-by-zero.\n",
    "\n",
    "    Returns:\n",
    "        A tensor of size [...] with SNR computed between matching slices of the\n",
    "        input signal and noise tensors.\n",
    "    \"\"\"\n",
    "    # Pre-multiplication and change of logarithm base.\n",
    "    constant = tf.cast(10.0 / tf.math.log(10.0), signal_power.dtype)\n",
    "\n",
    "    return constant * tf.math.log(tf.math.truediv(signal_power + epsilon, noise_power + epsilon))\n",
    "\n",
    "\n",
    "def calculate_signal_to_noise_ratio(signal, noise, epsilon=1e-8):\n",
    "    \"\"\"Computes the signal to noise ratio given signal and noise.\n",
    "\n",
    "    Args:\n",
    "        signal: A [..., samples] tensor of unknown shape and arbitrary rank.\n",
    "        noise: A tensor matching the signal tensor.\n",
    "        epsilon: An optional float for numerical stability, since silences\n",
    "          can lead to divide-by-zero.\n",
    "\n",
    "    Returns:\n",
    "          A tensor of size [...] with SNR computed between matching slices of the\n",
    "        input signal and noise tensors.\n",
    "    \"\"\"\n",
    "    def power(x):\n",
    "        return tf.math.reduce_mean(tf.square(source_wave), axis=-1)\n",
    "    \n",
    "\n",
    "    return calculate_signal_to_noise_ratio_from_power(power(signal), power(noise), epsilon)\n",
    "\n",
    "\n",
    "def signal_to_noise_ratio_gain_invariant(estimate, target, epsilon=1e-8):\n",
    "    \"\"\"Computes the signal to noise ratio in a gain invariant manner.\n",
    "\n",
    "      This computes SNR assuming that the signal equals the target multiplied by an\n",
    "      unknown gain, and that the noise is orthogonal to the target.\n",
    "\n",
    "      This quantity is also known as SI-SDR [1, equation 5].\n",
    "\n",
    "      This function estimates SNR using a formula given e.g. in equation 4.38 from\n",
    "      [2], which gives accurate results on a wide range of inputs, and yields a\n",
    "      monotonically decreasing value when target or estimate scales toward zero.\n",
    "\n",
    "      [1] Jonathan Le Roux, Scott Wisdom, Hakan Erdogan, John R. Hershey,\n",
    "      \"SDR--half-baked or well done?\",ICASSP 2019,\n",
    "      https://arxiv.org/abs/1811.02508.\n",
    "      [2] Magnus Borga, \"Learning Multidimensional Signal Processing\"\n",
    "      https://www.diva-portal.org/smash/get/diva2:302872/FULLTEXT01.pdf\n",
    "\n",
    "      Args:\n",
    "        estimate: An estimate of the target of size [..., samples].\n",
    "        target: A ground truth tensor, matching estimate above.\n",
    "        epsilon: An optional float introduced for numerical stability in the\n",
    "          projections only.\n",
    "\n",
    "      Returns:\n",
    "        A tensor of size [...] with SNR computed between matching slices of the\n",
    "        input signal and noise tensors.\n",
    "    \"\"\"\n",
    "    def normalize(x):\n",
    "        power = tf.math.reduce_sum(tf.square(x), keepdims=True, axis=[-1])\n",
    "        return tf.math.multiply(x, tf.math.rsqrt(tf.math.maximum(power, 1e-16)))\n",
    "\n",
    "    normalized_estimate = normalize(estimate)\n",
    "    normalized_target = normalize(target)\n",
    "    \n",
    "    cosine_similarity = tf.math.reduce_sum(tf.math.multiply(normalized_estimate, normalized_target),axis=[-1])\n",
    "    squared_cosine_similarity = tf.math.square(cosine_similarity)\n",
    "    normalized_signal_power = squared_cosine_similarity\n",
    "    normalized_noise_power = 1. - squared_cosine_similarity\n",
    "\n",
    "    # Computing normalized_noise_power as the difference between very close\n",
    "    # floating-point numbers is not accurate enough for this case, so when\n",
    "    # normalized_signal power is close to 0., we use an alternate formula.\n",
    "    # Both formulas are accurate enough at the 'seam' in float32.\n",
    "    normalized_noise_power_direct = tf.math.reduce_sum(\n",
    "              tf.math.square(normalized_estimate -\n",
    "                normalized_target * tf.expand_dims(cosine_similarity, -1)),axis=[-1])\n",
    "    \n",
    "    normalized_noise_power = tf.where(\n",
    "        tf.greater_equal(normalized_noise_power, 0.01),\n",
    "        normalized_noise_power,\n",
    "        normalized_noise_power_direct)\n",
    "\n",
    "    return calculate_signal_to_noise_ratio_from_power(\n",
    "        normalized_signal_power, normalized_noise_power, epsilon)\n",
    "\n",
    "\n",
    "def signal_to_noise_ratio_residual(estimate, target, epsilon=1e-8):\n",
    "    \"\"\"Computes the signal to noise ratio using residuals.\n",
    "\n",
    "    This computes the SNR in a \"statistical fashion\" as the logarithm of the\n",
    "    relative residuals. The signal is defined as the original target, and the\n",
    "    noise is the residual between the estimate and the target. This is\n",
    "    proportional to log(1 - 1/R^2).\n",
    "\n",
    "    Args:\n",
    "        estimate: An estimate of the target of size [..., samples].\n",
    "        target: A ground truth tensor, matching estimate above.\n",
    "        epsilon: An optional float for numerical stability, since silences\n",
    "        can lead to divide-by-zero.\n",
    "\n",
    "    Returns:\n",
    "        A tensor of size [...] with SNR computed between matching slices of the\n",
    "        input signal and noise tensors.\n",
    "    \"\"\"\n",
    "    return calculate_signal_to_noise_ratio(target, target - estimate, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "304d10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weights_for_nonzero_refs(source_waveforms):\n",
    "    \"\"\"Return shape (source,) weights for signals that are nonzero.\"\"\"\n",
    "    source_norms = tf.sqrt(tf.reduce_mean(tf.square(source_waveforms), axis=-1))\n",
    "    return tf.greater(source_norms, 1e-8)\n",
    "\n",
    "def _weights_for_active_seps(power_sources, power_separated):\n",
    "    \"\"\"Return (source,) weights for active separated signals.\"\"\"\n",
    "    min_power = tf.reduce_min(power_sources, axis=-1, keepdims=True)\n",
    "    return tf.greater(power_separated, 0.01 * min_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e345e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_metrics(source_waveforms, separated_waveforms, mixture_waveform):\n",
    "    \"\"\"Permutation-invariant SI-SNR, powers, and under/equal/over-separation.\"\"\"\n",
    "    perm_inv_loss = wrap(lambda tar, est: -signal_to_noise_ratio_gain_invariant(est, tar))\n",
    "    _, separated_waveforms = perm_inv_loss(source_waveforms,separated_waveforms)\n",
    "      \n",
    "    # Compute separated and source powers.\n",
    "    power_separated = tf.reduce_mean(separated_waveforms ** 2, axis=-1)\n",
    "    power_sources = tf.reduce_mean(source_waveforms ** 2, axis=-1)\n",
    "    \n",
    "    # Compute weights for active (separated, source) pairs where source is nonzero\n",
    "    # and separated power is above threshold of quietest source power - 20 dB.\n",
    "    weights_active_refs = _weights_for_nonzero_refs(source_waveforms)\n",
    "    weights_active_seps = _weights_for_active_seps(\n",
    "        tf.boolean_mask(power_sources, weights_active_refs), power_separated)\n",
    "    weights_active_pairs = tf.logical_and(weights_active_refs,\n",
    "                                        weights_active_seps)\n",
    "    \n",
    "    # Compute SI-SNR.\n",
    "    sisnr_separated = signal_to_noise_ratio_gain_invariant(separated_waveforms, source_waveforms)\n",
    "    num_active_refs = tf.math.reduce_sum(tf.cast(weights_active_refs, tf.int32))\n",
    "    num_active_seps = tf.math.reduce_sum(tf.cast(weights_active_seps, tf.int32))\n",
    "    num_active_pairs = tf.math.reduce_sum(tf.cast(weights_active_pairs, tf.int32))\n",
    "    sisnr_mixture = signal_to_noise_ratio_gain_invariant(\n",
    "      tf.tile(mixture_waveform, (1,source_waveforms.shape[1], 1)),source_waveforms)\n",
    "    \n",
    "    # Compute under/equal/over separation.\n",
    "    under_separation = tf.cast(tf.less(num_active_seps, num_active_refs),\n",
    "                             tf.float32)\n",
    "    equal_separation = tf.cast(tf.equal(num_active_seps, num_active_refs),\n",
    "                             tf.float32)\n",
    "    over_separation = tf.cast(tf.greater(num_active_seps, num_active_refs),\n",
    "                            tf.float32)\n",
    "    \n",
    "    return {'sisnr_separated': sisnr_separated,\n",
    "          'sisnr_mixture': sisnr_mixture,\n",
    "          'sisnr_improvement': sisnr_separated - sisnr_mixture,\n",
    "          'power_separated': power_separated,\n",
    "          'power_sources': power_sources,\n",
    "          'under_separation': under_separation,\n",
    "          'equal_separation': equal_separation,\n",
    "          'over_separation': over_separation,\n",
    "          'weights_active_refs': weights_active_refs,\n",
    "          'weights_active_seps': weights_active_seps,\n",
    "          'weights_active_pairs': weights_active_pairs,\n",
    "          'num_active_refs': num_active_refs,\n",
    "          'num_active_seps': num_active_seps,\n",
    "          'num_active_pairs': num_active_pairs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27518136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _report_score_stats(metric_per_source_count, label='', counts=None):\n",
    "    \"\"\"Report mean and std dev for specified counts.\"\"\"\n",
    "    values_all = []\n",
    "    if counts is None:\n",
    "        counts = metric_per_source_count.keys()\n",
    "    for count in counts:\n",
    "        values = metric_per_source_count[count]\n",
    "        values_all.extend(list(values))\n",
    "    return '%s for count(s) %s = %.1f +/- %.1f dB' % (label, counts, np.mean(values_all), np.std(values_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eecb8ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFinalMetricsFuss(sources,sep_wave,mix_wave,n_samples):\n",
    "    i=1\n",
    "    max_count = 4\n",
    "    dict_per_source_count = lambda: {c: [] for c in range(1, max_count + 1)}\n",
    "    sisnr_per_source_count = dict_per_source_count()\n",
    "    sisnri_per_source_count = dict_per_source_count()\n",
    "    under_seps = []\n",
    "    equal_seps = []\n",
    "    over_seps = []\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            metrics_dict = metrics = compute_final_metrics(source_wave[i,:,:][np.newaxis,:],\\\n",
    "                                sep_wave[i,:,:][np.newaxis,:],mix_wave[i,:,:][np.newaxis,:])\n",
    "\n",
    "            metrics_dict = {k: v.numpy() for k, v in metrics_dict.items()}\n",
    "            sisnr_sep = metrics_dict['sisnr_separated']\n",
    "            sisnr_mix = metrics_dict['sisnr_mixture']\n",
    "            sisnr_imp = metrics_dict['sisnr_improvement']\n",
    "            weights_active_pairs = metrics_dict['weights_active_pairs']\n",
    "\n",
    "            # Store metrics per source count and report results so far.\n",
    "            under_seps.append(metrics_dict['under_separation'])\n",
    "            equal_seps.append(metrics_dict['equal_separation'])\n",
    "            over_seps.append(metrics_dict['over_separation'])\n",
    "            sisnr_per_source_count[metrics_dict['num_active_refs']].extend(\n",
    "            sisnr_sep[weights_active_pairs].tolist())\n",
    "            sisnri_per_source_count[metrics_dict['num_active_refs']].extend(\n",
    "                    sisnr_imp[weights_active_pairs].tolist())\n",
    "\n",
    "            # Report mean statistics and save csv every so often.\n",
    "            lines = [\n",
    "                      'Metrics after %d examples:' % i,\n",
    "                      _report_score_stats(sisnr_per_source_count, 'SI-SNR',\n",
    "                                          counts=[1]),\n",
    "                      _report_score_stats(sisnri_per_source_count, 'SI-SNRi',\n",
    "                                          counts=[2]),\n",
    "                      _report_score_stats(sisnri_per_source_count, 'SI-SNRi',\n",
    "                                          counts=[3]),\n",
    "                      _report_score_stats(sisnri_per_source_count, 'SI-SNRi',\n",
    "                                          counts=[4]),\n",
    "                      _report_score_stats(sisnri_per_source_count, 'SI-SNRi',\n",
    "                                          counts=[2, 3, 4]),\n",
    "                      'Under separation: %.2f' % np.mean(under_seps),\n",
    "                      'Equal separation: %.2f' % np.mean(equal_seps),\n",
    "                      'Over separation: %.2f' % np.mean(over_seps),\n",
    "            ]\n",
    "\n",
    "        print('')\n",
    "        for line in lines:\n",
    "            print(line)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5587c31",
   "metadata": {},
   "source": [
    "## Example of final metrics over fuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d8e1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 5) float32\n",
      "(3, 4, 5) float32\n",
      "(3, 1, 5) float32\n",
      "\n",
      "Metrics after 2 examples:\n",
      "SI-SNR for count(s) [1] = 80.0 +/- 0.0 dB\n",
      "SI-SNRi for count(s) [2] = nan +/- nan dB\n",
      "SI-SNRi for count(s) [3] = 27.1 +/- 32.0 dB\n",
      "SI-SNRi for count(s) [4] = 29.1 +/- 29.3 dB\n",
      "SI-SNRi for count(s) [2, 3, 4] = 28.1 +/- 30.7 dB\n",
      "Under separation: 0.33\n",
      "Equal separation: 0.33\n",
      "Over separation: 0.33\n"
     ]
    }
   ],
   "source": [
    "# Batch Size is 2\n",
    "# Length is 5\n",
    "# Max source is 3\n",
    "\n",
    "source_wave = np.array([[[1,5,7,3,5],[2,5,8,3,6],[0,0,0,0,0],[1,1,1,1,1]],[[.7,.7,.7,.7,.7],\\\n",
    "                        [1,4,6,4,3],[2,4,7,4,2],[2,2,2,2,2]],[[1,1,1,1,1],[0,0,0,0,0],\\\n",
    "                            [0,0,0,0,0],[0,0,0,0,0]]],dtype='float32')\n",
    "print(source_wave.shape,source_wave.dtype)\n",
    "\n",
    "sep_wave = np.array([[[1.4,5.4,7.3,3.1,5.2],[2.1,5.7,8.3,3.4,6.6],[.1,.1,.1,.1,.1],\\\n",
    "                      [1,1,1,1,1]],[[1,1,1,1,1],[1.5,4.3,6.6,4.3,3.4],[2.5,4.7,7.3,4.6,2.4],\\\n",
    "                      [0,0,0,0,0]],[[1,1,1,1,1],[0,0,0,0,0],\\\n",
    "                       [0,0,0,0,0],[0,0,0,0,0]]],dtype='float32')\n",
    "print(sep_wave.shape,sep_wave.dtype)\n",
    "\n",
    "mix_wave = np.sum(source_wave,axis=1,keepdims=True)\n",
    "print(mix_wave.shape,mix_wave.dtype)\n",
    "\n",
    "getFinalMetricsFuss(source_wave,sep_wave,mix_wave,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918c170",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6426816e",
   "metadata": {},
   "source": [
    "$$ \n",
    "L(s, \\hat{s})= \\min_{\\pi \\in \\Pi} [ \\sum_{m_a = 1}^{M_a} L_{SNR}(s_{m_{a}},\\hat{s_{\\pi (m_a)}}) + \\sum_{m_o = M_a + 1}^{M} L_o (x, \\hat{s_{\\pi (m_o )}} )]\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{SNR}(y, \\hat{y}) = 10 \\log_{10} ( ||y-\\hat{y}||^2 + \\tau ||y||^2 )\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{o}(y, \\hat{y}) = 10 \\log_{10} ( ||y-\\hat{y}||^2 + \\tau ||x||^2 )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca91b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_or_dynamic_dim_size(tensor, i):\n",
    "    \"\"\"Static size for dimension `i` if available, otherwise dynamic size.\"\"\"\n",
    "    static_shape = tensor.shape\n",
    "    dyn_shape = tf.shape(tensor)\n",
    "    return (static_shape[i].value if hasattr(static_shape[i], 'value')\n",
    "          else static_shape[i]) or dyn_shape[i]\n",
    "\n",
    "\n",
    "def smart_shape(tensor):\n",
    "    \"\"\"Shape of tensor with static and/or dynamic dimensions.\n",
    "\n",
    "    Args:\n",
    "        tensor: A tf.Tensor.\n",
    "\n",
    "    Returns:\n",
    "        A list containing static (type int) and dynamic (tf.Tensor) dim sizes.\n",
    "    \"\"\"\n",
    "    dims = []\n",
    "    for i in range(len(tensor.shape)):\n",
    "        dims.append(static_or_dynamic_dim_size(tensor, i))\n",
    "    return tuple(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1147884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stabilized_log_base(x, base=10., stabilizer=1e-8):\n",
    "    \"\"\"Stabilized log with specified base.\"\"\"\n",
    "    logx = tf.math.log(x + stabilizer)\n",
    "    logb = tf.math.log(tf.constant(base, dtype=logx.dtype))\n",
    "    return logx / logb\n",
    "\n",
    "def log_mse_loss(source, separated, max_snr=1e6, bias_ref_signal=None):\n",
    "    \"\"\"Negative log MSE loss, the negated log of SNR denominator.\"\"\"\n",
    "    err_pow = tf.math.reduce_sum(tf.math.square(source - separated), axis=-1)\n",
    "    snrfactor = 10.**(-max_snr / 10.)\n",
    "    if bias_ref_signal is None:\n",
    "        ref_pow = tf.math.reduce_sum(tf.square(source), axis=-1)\n",
    "    else:\n",
    "        ref_pow = tf.math.reduce_sum(tf.math.square(bias_ref_signal), axis=-1)\n",
    "    bias = snrfactor * ref_pow\n",
    "    return 10. * _stabilized_log_base(bias + err_pow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a75c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupwise_apply(loss_fns: typing.Dict[str, typing.Callable[..., typing.Any]],\n",
    "          signal_names: typing.List[str],\n",
    "          reference: tf.Tensor,\n",
    "          estimate: tf.Tensor,\n",
    "          permutation_invariant_losses: typing.List[str]):\n",
    "    \"\"\"Apply loss functions to the corresponding references and estimates.\n",
    "\n",
    "    For each kind of signal, gather corresponding references and estimates, and\n",
    "    apply the loss function.  Scatter-add the results into the loss.\n",
    "\n",
    "    For elements of signals_names not in loss_fns, no loss will be applied.\n",
    "\n",
    "    Args:\n",
    "        loss_fns: dictionary of string -> loss_fn.\n",
    "            Each string is a name to match elements of signal_names.\n",
    "            Each loss_fn has the following signature:\n",
    "        Args\n",
    "            reference [batch, grouped_source, ...] tensor\n",
    "            estimate [batch, grouped_source, ...] tensor\n",
    "    Returns\n",
    "        A [batch, grouped_source] tensor of dtype=tf.float32\n",
    "        signal_names: list of names of each signal.\n",
    "        reference: [batch, source, ...] tensor.\n",
    "        estimate: [batch, source, ...] tensor.\n",
    "        permutation_invariant_losses: List of losses to be permutation invariant.\n",
    "\n",
    "    Returns:\n",
    "        loss, A [batch, source] tensor of dtype=tf.float32\n",
    "    \"\"\"\n",
    "    if reference.shape[:2] != estimate.shape[:2]:\n",
    "        raise ValueError('First two axes (batch, source) of reference and estimate'\n",
    "                     'must be equal, got {}, {}'.format(\n",
    "                         reference.shape[:2], estimate.shape[:2]))\n",
    "        \n",
    "    batch = reference.shape[0]\n",
    "    loss = tf.zeros(shape=reference.shape[:2], dtype=tf.float32)\n",
    "    permuted_estimates = tf.zeros_like(reference)\n",
    "\n",
    "    # For each kind of signal, e.g. 'speech', 'noise', gather subsets of reference\n",
    "    # and estimate, apply loss function and scatter-add into the loss tensor.\n",
    "    for name, loss_fn in loss_fns.items():\n",
    "        print(name)\n",
    "\n",
    "        idxs = [idx for idx, value in enumerate(signal_names) if value == name]\n",
    "        idxs_0 = tf.tile(\n",
    "            tf.expand_dims(tf.range(batch), 1),\n",
    "            [1, len(idxs)])\n",
    "        idxs_1 = tf.tile(\n",
    "            tf.expand_dims(tf.constant(idxs, dtype=tf.int32), 0),\n",
    "            [batch, 1])\n",
    "        \n",
    "        idxs_nd = tf.stack([idxs_0, idxs_1], axis=2)\n",
    "        reference_key = tf.gather_nd(reference, idxs_nd)\n",
    "        estimate_key = tf.gather_nd(estimate, idxs_nd)\n",
    "        \n",
    "        loss_fn = wrap(\n",
    "            loss_fn,\n",
    "            enable=name in permutation_invariant_losses)\n",
    "        loss_key, permuted_estimates_key = loss_fn(reference_key, estimate_key)\n",
    "        \n",
    "        loss = tf.tensor_scatter_nd_add(loss, idxs_nd, loss_key)\n",
    "        permuted_estimates = tf.tensor_scatter_nd_add(\n",
    "            permuted_estimates, idxs_nd, permuted_estimates_key)\n",
    "        \n",
    "    return loss, permuted_estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFussLoss(mixture_waveforms,source_waveforms,separated_waveforms,batch_size):\n",
    "    hparams_signal_types = ['source'] * 4\n",
    "    unique_signal_types = list(set(hparams_signal_types))\n",
    "    loss_fns = {signal_type: log_mse_loss for signal_type in unique_signal_types}\n",
    "\n",
    "    _, separated_waveforms = groupwise_apply(loss_fns,\n",
    "                                            hparams_signal_types,\n",
    "                                            source_waveforms,\n",
    "                                            separated_waveforms,\n",
    "                                            unique_signal_types)\n",
    "\n",
    "    # Build loss split between all-zero and nonzero reference signals.\n",
    "    source_is_nonzero = _weights_for_nonzero_refs(source_waveforms)\n",
    "    source_is_zero = tf.math.logical_not(source_is_nonzero)\n",
    "\n",
    "    # Get batch size and (max) number of sources.\n",
    "    num_sources = 4\n",
    "\n",
    "    # Waveforms with nonzero references.\n",
    "    source_waveforms_nonzero = tf.boolean_mask(\n",
    "          source_waveforms, source_is_nonzero)[:, tf.newaxis]\n",
    "    separated_waveforms_nonzero = tf.boolean_mask(\n",
    "          separated_waveforms, source_is_nonzero)[:, tf.newaxis]\n",
    "\n",
    "    # Waveforms with all-zero references.\n",
    "    source_waveforms_zero = tf.boolean_mask(\n",
    "          source_waveforms, source_is_zero)[:, tf.newaxis]\n",
    "    separated_waveforms_zero = tf.boolean_mask(\n",
    "          separated_waveforms, source_is_zero)[:, tf.newaxis]\n",
    "\n",
    "    weight = 1. / tf.cast(batch_size * num_sources, tf.float32)\n",
    "\n",
    "    mixture_waveforms_zero = tf.boolean_mask(\n",
    "            tf.tile(mixture_waveforms[:, 0:1], (1, num_sources, 1)),\n",
    "            source_is_zero)[:, tf.newaxis]\n",
    "    loss = tf.math.reduce_sum(log_mse_loss(source_waveforms_zero,\n",
    "                                          separated_waveforms_zero,\n",
    "                                          max_snr=20,\n",
    "                                          bias_ref_signal=mixture_waveforms_zero))\n",
    "    loss_zero = tf.identity(1 * weight * loss, name='loss_ref_zero')\n",
    "\n",
    "    # Loss for nonzero references.\n",
    "    loss = tf.math.reduce_sum(log_mse_loss(source_waveforms_nonzero,\n",
    "                                        separated_waveforms_nonzero,\n",
    "                                        max_snr=30))\n",
    "    loss_nonzero = tf.identity(weight * loss, name='loss_ref_nonzero')\n",
    "    \n",
    "    return loss_zero+loss_nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a3c33",
   "metadata": {},
   "source": [
    "## Example of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_waveforms = np.array([[[1,5],[2,5],[0,0],[1,1]],\n",
    "                            [[0,0],[2,5],[0,0],[1,1]]],dtype='float32')\n",
    "print(source_waveforms.shape) \n",
    "\n",
    "separated_waveforms = np.array([[[2.1,5.7],[1.4,5.4],\n",
    "                                 [1,1],[.1,.1]],[[0,0],[1.4,5.4],\n",
    "                                 [1,1],[.1,.1]]],dtype='float32')\n",
    "print(separated_waveforms.shape)\n",
    "\n",
    "mixture_waveforms = np.sum(source_waveforms,keepdims=True,axis=1)\n",
    "print(mixture_waveforms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = getFussLoss(mixture_waveforms,source_waveforms,separated_waveforms,2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb88ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc38145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce163a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59962aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a70e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfp-env",
   "language": "python",
   "name": "anaconda-tfp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
