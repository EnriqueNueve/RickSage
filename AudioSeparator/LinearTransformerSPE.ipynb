{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cdf6f07",
   "metadata": {},
   "source": [
    "# Linear Transformer with SPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751eaba5",
   "metadata": {},
   "source": [
    "## Import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da083e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509708d2",
   "metadata": {},
   "source": [
    "## Linear Multi Head Attention with SineSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b0ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineSPE(layers.Layer):\n",
    "    def __init__(self, \n",
    "                 num_heads: int = 8,\n",
    "                 in_features: int = 64,\n",
    "                 num_realizations: int = 256,\n",
    "                 num_sines: int = 1):\n",
    "        super(SineSPE, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.in_features = in_features \n",
    "        self.num_sines = num_sines \n",
    "        self.num_realizations = num_realizations\n",
    "        \n",
    "        freqs_init = tf.random_normal_initializer()\n",
    "        self.freqs = tf.Variable(\n",
    "            initial_value=freqs_init(shape=(num_heads, in_features, num_sines), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        offsets_init = tf.random_normal_initializer()\n",
    "        self.offsets = tf.Variable(\n",
    "            initial_value=offsets_init(shape=(num_heads, in_features, num_sines), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        gains_init = tf.random_normal_initializer()\n",
    "        self.gains = tf.Variable(\n",
    "            initial_value=gains_init(shape=(num_heads, in_features, num_sines), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        # Normalize gains \n",
    "        self.gains = self.gains/(tf.math.sqrt(tf.norm(self.gains,axis=-1,keepdims=True))/2)\n",
    "        \n",
    "        # Bias intial freqs\n",
    "        self.freqs = self.freqs-4\n",
    "        \n",
    "        self.code_shape = (num_heads,in_features)\n",
    "\n",
    "    def call(self, shape):\n",
    "        \"\"\"\n",
    "        Generate the code, composed of a random QBar and Kbar,\n",
    "        depending on the parameters, and return them for use with a\n",
    "        SPE module to actually encode queries and keys.\n",
    "        Args:\n",
    "            shape: The outer shape of the inputs: (batchsize, *size)\n",
    "            num_realizations: if provided, overrides self.num_realizations\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(shape) != 2:\n",
    "            raise ValueError('Only 1D inputs are supported by SineSPE')\n",
    "        \n",
    "        max_len = shape[1]\n",
    "        \n",
    "        # build omega_q and omega_k\n",
    "        # with shape (num_heads,keys_dim,length,2*num_sines)\n",
    "        indices = tf.linspace(0,max_len-1,max_len)\n",
    "        indices = tf.cast(indices, dtype=tf.float32)\n",
    "\n",
    "        # make sure freqs are in [0,.5]\n",
    "        freqs = tf.nn.sigmoid(self.freqs[:,:,None,:])/2\n",
    "        \n",
    "        phases_q = 2*math.pi*freqs*indices[None,None,:,None]*self.offsets[:,:,None,:]\n",
    "        omega_q = tf.stack([tf.math.cos(phases_q),tf.math.sin(phases_q)],axis=-1)\n",
    "        omega_q = tf.reshape(omega_q,[1,self.num_heads,self.in_features,max_len,2*self.num_sines] )\n",
    "        \n",
    "        phases_k = 2*math.pi*freqs*indices[None,None,:,None]\n",
    "        omega_k = tf.stack([tf.math.cos(phases_k),tf.math.sin(phases_k)],axis=-1)\n",
    "        omega_k = tf.reshape(omega_k,[1,self.num_heads,self.in_features,max_len,2*self.num_sines] )\n",
    "        \n",
    "        # Gains is (num_heads,keys_dim,num_sines), make nonnegative with softplut\n",
    "        gains = tf.math.softplus(self.gains)\n",
    "        \n",
    "        # Upsample\n",
    "        gains = tf.stack([gains,gains],axis=-1)\n",
    "        gains = tf.reshape(gains, [self.num_heads,self.in_features,2*self.num_sines])\n",
    "        \n",
    "        # Draw noise\n",
    "        z = tf.random.normal((1,self.num_heads,self.in_features,2*self.num_sines,self.num_realizations))\n",
    "        z = z/tf.math.sqrt(tf.cast(self.num_sines*2, dtype=tf.float32))\n",
    "        \n",
    "        # Scale each of the 2*num_sines by the appropriate gain\n",
    "        z = z*gains[None, ..., None]\n",
    "    \n",
    "        # Compute sums over sines\n",
    "        qbar = tf.linalg.matmul(omega_q,z)\n",
    "        kbar = tf.linalg.matmul(omega_k,z)\n",
    "        \n",
    "        # Pemute to (1,length,num_heads,key_dim,num_realization)\n",
    "        qbar = tf.transpose(qbar, perm=[0,3,1,2,4])\n",
    "        kbar = tf.transpose(kbar, perm=[0,3,1,2,4])\n",
    "\n",
    "        # scale\n",
    "        scale = (self.num_realizations*self.in_features)**.25\n",
    "        return (qbar/scale,kbar/scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1999e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPEFilter(layers.Layer):\n",
    "    \"\"\"Stochastic positional encoding filter\n",
    "    Applies a positional code provided by a SPE module on actual queries and keys.\n",
    "    Implements gating, i.e. some \"dry\" parameter, that lets original queries and keys through if activated.\n",
    "    Args:\n",
    "    gated: whether to use the gated version, which learns to balance\n",
    "        positional and positionless features.\n",
    "    code_shape: the inner shape of the codes, i.e. (num_heads, key_dim),\n",
    "        as given by `spe.code_shape`\n",
    "    \"\"\"\n",
    "    def __init__(self,gated,code_shape):\n",
    "        super(SPEFilter, self).__init__()\n",
    "\n",
    "        self.gated = gated\n",
    "        self.code_shape = code_shape\n",
    "\n",
    "        # create the gating parameters if required\n",
    "        if gated:\n",
    "            if code_shape is None:\n",
    "                raise RuntimeError('code_shape has to be provided if gated is True.')\n",
    "\n",
    "            gate_init = tf.random_normal_initializer()\n",
    "            self.gate = tf.Variable(\n",
    "                initial_value=gate_init(shape=(code_shape), dtype=\"float32\"),\n",
    "                trainable=True,\n",
    "            )  \n",
    "\n",
    "    def call(self,queries,keys,code):\n",
    "        \"\"\"\n",
    "        Apply SPE on keys with a given code.\n",
    "        Expects keys and queries of shape `(batch_size, ..., num_heads,\n",
    "        key_dim)` and outputs keys and queries of shape `(batch_size,\n",
    "        ..., num_heads, num_realizations)`. code is the tuple\n",
    "        of the 2 tensors provided by the code instance, each one of\n",
    "        shape (1, ..., num_heads, key_dim, num_realizations)\n",
    "        \"\"\"\n",
    "        assert (queries.shape == keys.shape), \\\n",
    "            \"As of current implementation, queries and keys must have the same shape. \"\\\n",
    "            \"got queries: {} and keys: {}\".format(queries.shape, keys.shape)\n",
    "\n",
    "        # qbar and kbar are (1, *shape, num_heads, keys_dim, num_realizations)\n",
    "        (qbar, kbar) = code\n",
    "\n",
    "        # check that codes have the shape we are expecting\n",
    "        if self.code_shape is not None and qbar.shape[-3:-1] != self.code_shape:\n",
    "            raise ValueError(\n",
    "                f'The inner shape of codes is {qbar.shape[-3:-1]}, '\n",
    "                f'but expected {self.code_shape}')\n",
    "\n",
    "        # check shapes: size of codes should be bigger than queries, keys\n",
    "        code_size = qbar.shape[1:-3]\n",
    "        query_size = queries.shape[1:-2]\n",
    "        if (len(code_size) != len(query_size)\n",
    "            or tf.reduce_any(\n",
    "                tf.Variable(code_size) < tf.Variable(query_size)\n",
    "            )):\n",
    "                raise ValueError(f'Keys/queries have length {query_size}, '\n",
    "                                 f'but expected at most {code_size}')\n",
    "        if qbar.shape[-3:-1] != queries.shape[-2:]:\n",
    "            raise ValueError(f'shape mismatch. codes have shape {qbar.shape}, '\n",
    "                             f'but queries are {queries.shape}')\n",
    "\n",
    "        # truncate qbar and kbar for matching current queries and keys,\n",
    "        # but only if we need to\n",
    "        for dim in range(len(query_size)):\n",
    "            if code_size[dim] > query_size[dim]:\n",
    "                indices = [slice(1), *[slice(qbar.shape[1+k]) for k in range(dim)],\n",
    "                           slice(query_size[dim])]\n",
    "                qbar = qbar[indices]\n",
    "                kbar = kbar[indices]\n",
    "\n",
    "        # apply gate if required\n",
    "        if self.gated:\n",
    "            # incorporate the constant bias for Pd if required. First draw noise\n",
    "            # such that noise noise^T = 1, for each head, feature, realization.\n",
    "            # qbar is : (1, *shape, num_heads, keys_dim, num_realizations)\n",
    "            in_features = qbar.shape[-2]\n",
    "            num_realizations = qbar.shape[-1]\n",
    "            gating_noise = tf.random.normal(self.code_shape+\\\n",
    "                            (num_realizations,))/(in_features*num_realizations)**.25\n",
    "            \n",
    "            \n",
    "            # normalize it so that it's an additive 1 to Pd\n",
    "            #gating_noise = gating_noise / gating_noise.norm(dim=2, keepdim=True)\n",
    "\n",
    "            # constrain the gate parameter to be in [0 1]\n",
    "            gate = tf.math.sigmoid(self.gate[..., None])\n",
    "\n",
    "            # qbar is (1, *shape, num_heads, keys_dim, num_realizations)\n",
    "            # gating noise is (num_heads, keys_dim, num_realizations)\n",
    "            # gate is (num_heads, keys_dim, 1)\n",
    "            #import ipdb; ipdb.set_trace()\n",
    "            qbar = tf.math.sqrt(1.-gate) * qbar  + tf.math.sqrt(gate) * gating_noise\n",
    "            kbar = tf.math.sqrt(1.-gate) * kbar  + tf.math.sqrt(gate) * gating_noise\n",
    "\n",
    "        # sum over d after multiplying by queries and keys\n",
    "        # qbar/kbar are (1, *shape, num_heads, keys_dim, num_realizations)\n",
    "        # queries/keys  (batchsize, *shape, num_heads, keys_dim)\n",
    "        qhat = tf.math.reduce_sum(qbar * queries[..., None],axis=-2)\n",
    "        khat = tf.math.reduce_sum(kbar * keys[..., None],axis=-2)\n",
    "\n",
    "        # result is (batchsize, ..., num_heads, num_realizations)\n",
    "        return (qhat, khat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "443f068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linear_mhsa(q, k, v):\n",
    "    q = tf.nn.elu(q)+1\n",
    "    k = tf.nn.elu(k)+1\n",
    "    kv = tf.einsum('... h s d, ...  h s m  -> ... h m d',k,v)\n",
    "    k_sum = tf.math.reduce_sum(k,axis=2)\n",
    "    z = 1/ (tf.einsum('... h l d, ... h d -> ... h l',q ,k_sum)+1e-4)\n",
    "    Vhat = tf.einsum('... h l d, ... h m d, ... h l -> ... h l m',q,kv,z)\n",
    "    return Vhat\n",
    "\n",
    "class LinearAttentionSineSPE(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, heads=8, num_sines=5):\n",
    "        super(LinearAttentionSineSPE, self).__init__()\n",
    "        self.num_heads = heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.spe_encoder = SineSPE(num_heads=heads,          # Number of attention heads\n",
    "                          in_features=self.depth,       # Dimension of keys and queries\n",
    "                          num_realizations=self.depth,  # New dimension of keys and queries\n",
    "                          num_sines=num_sines)          # Number of sinusoidal components\n",
    "        self.spe_filter = SPEFilter(gated=True, code_shape=self.spe_encoder.code_shape)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)   \n",
    "\n",
    "        q = tf.transpose(q,perm=[0,2,1,3])\n",
    "        k = tf.transpose(k,perm=[0,2,1,3])\n",
    "        \n",
    "        pos_codes = self.spe_encoder(q.shape[:2])  # pos_codes is a tuple (qbar, kbar)\n",
    "        q, k = self.spe_filter(q, k, pos_codes)\n",
    "        q = tf.transpose(q,perm=[0,2,1,3])\n",
    "        k = tf.transpose(k,perm=[0,2,1,3])\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention = compute_linear_mhsa(q, k, v)\n",
    "\n",
    "        scaled_attention = tf.transpose(\n",
    "            scaled_attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(\n",
    "            scaled_attention, (batch_size, -1, self.d_model)\n",
    "        )  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad74e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 64)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "seq_len = 100\n",
    "n_heads = 8\n",
    "embedding_dim = 64 \n",
    "\n",
    "x = np.random.uniform(0,1,(BATCH_SIZE,seq_len,embedding_dim))\n",
    "lha = LinearAttentionSineSPE(embedding_dim ,n_heads)\n",
    "\n",
    "y = lha(x,x,x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904df3a1",
   "metadata": {},
   "source": [
    "## Linear Multi Head Attention with SineSPE Transformer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e06ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSineSPETransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        \n",
    "        super(LinearSineSPETransformerBlock, self).__init__()\n",
    "\n",
    "        self.lha = LinearAttentionSineSPE(embed_dim,num_heads)\n",
    "        \n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.lha(inputs,inputs,inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c05a8bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 64)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "seq_len = 100\n",
    "n_heads = 8\n",
    "embedding_dim = 64 \n",
    "ff_dim = 40\n",
    "\n",
    "x = np.random.uniform(0,1,(BATCH_SIZE,seq_len,embedding_dim))\n",
    "ltb = LinearSineSPETransformerBlock(embedding_dim, n_heads ,ff_dim)\n",
    "\n",
    "y = ltb(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1065e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e0ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99189328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7341b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ac6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0042838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de932bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfp-env",
   "language": "python",
   "name": "anaconda-tfp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
