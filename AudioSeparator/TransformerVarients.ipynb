{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41078057",
   "metadata": {},
   "source": [
    "# Transformer Varients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7d50fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e735aa",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6024d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    scale_factor =  tf.math.sqrt(tf.cast(tf.shape(k)[-1], tf.float32))\n",
    "    scaled_dot_prod = tf.einsum('... i d , ... j d -> ... i j', q, k) / scale_factor\n",
    "    attention_weights = tf.nn.softmax(scaled_dot_prod, axis=-1) \n",
    "    return tf.einsum('... i j , ... j d -> ... i d', attention_weights, v), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a829b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        print(\"q shape: {}\".format(q.shape))\n",
    "        print(\"k shape: {}\".format(k.shape))\n",
    "        print(\"v shape: {}\".format(v.shape))\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "        \n",
    "        print(\"q shape: {}\".format(q.shape))\n",
    "        print(\"k shape: {}\".format(k.shape))\n",
    "        print(\"v shape: {}\".format(v.shape))\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask\n",
    "        )\n",
    "\n",
    "        scaled_attention = tf.transpose(\n",
    "            scaled_attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(\n",
    "            scaled_attention, (batch_size, -1, self.d_model)\n",
    "        )  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f4c524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(50,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7330e245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q shape: (1, 100, 50)\n",
      "k shape: (1, 100, 50)\n",
      "v shape: (1, 100, 50)\n",
      "q shape: (1, 10, 100, 5)\n",
      "k shape: (1, 10, 100, 5)\n",
      "v shape: (1, 10, 100, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 100, 50])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.uniform(0,1,(1,100,50))\n",
    "y, _ = mha(x,x,x,mask=None)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b101b5",
   "metadata": {},
   "source": [
    "## Linformer Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cbca22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mhsa(q, k, v, scale_factor=1):\n",
    "    # resulted shape will be: [batch, heads, tokens, tokens]\n",
    "    scaled_dot_prod = tf.einsum('... i d , ... j d -> ... i j', q, k) * scale_factor\n",
    "    attention = tf.nn.softmax(scaled_dot_prod, axis=-1)\n",
    "    # calc result per head\n",
    "    return tf.einsum('... i j , ... j d -> ... i d', attention, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13fca5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_vk_linformer(v, k, E):\n",
    "    # project k,v\n",
    "    v = tf.einsum('b h j d , j k -> b h k d', v, E)\n",
    "    k = tf.einsum('b h j d , j k -> b h k d', k, E)\n",
    "    return v, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fcbe63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinformerAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, heads=8, dim_head=None, proj_shape=None, trainable_proj=True):\n",
    "        \"\"\"\n",
    "        Based on the Linformer paper\n",
    "        Link: https://arxiv.org/pdf/2006.04768.pdf\n",
    "        Args:\n",
    "            dim: token's dimension, i.e. word embedding vector size\n",
    "            heads: the number of distinct representations to learn\n",
    "            dim_head: the dim of the head.\n",
    "            shared_projection: if the projection matrix will be shared among layers\n",
    "            (it will have to be passed in the forward that way)\n",
    "            trainable_proj: if the projection matrix E matrix is not shared,\n",
    "            you can enable this option to make it trainable (non trainable in the paper)\n",
    "            proj_shape: 2-tuple (tokens,k), where k is the projection dimension of the linformer\n",
    "            \"\"\"\n",
    "        super(LinformerAttention, self).__init__()\n",
    "        self.dim_head = (int(dim / heads)) if dim_head is None else dim_head\n",
    "        _dim = self.dim_head * heads\n",
    "        self.heads = heads\n",
    "        \n",
    "        to_qvk_init = tf.random_normal_initializer()\n",
    "        self.to_qvk = tf.Variable(\n",
    "            initial_value= to_qvk_init(shape=(dim, _dim * 3), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        W_0_init = tf.random_normal_initializer()\n",
    "        self.W_0 = tf.Variable(\n",
    "            initial_value=W_0_init(shape=(_dim, dim), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.scale_factor = self.dim_head ** -0.5\n",
    "\n",
    "        E_init = tf.random_normal_initializer()\n",
    "        self.E = tf.Variable(initial_value=E_init(shape=(proj_shape), dtype=\"float32\"),\n",
    "                trainable=trainable_proj,) \n",
    "        self.k = proj_shape[1]\n",
    "\n",
    "    def call(self, x):\n",
    "        qkv = x@self.to_qvk # [batch, tokens, dim*3*heads ]\n",
    "\n",
    "        q, k, v = tuple(rearrange(qkv, 'b t (d k h ) -> k b h t d ', k=3, h=self.heads))\n",
    "\n",
    "        v, k = project_vk_linformer(v, k, self.E)\n",
    "\n",
    "        out = compute_mhsa(q, k, v, scale_factor=self.scale_factor)\n",
    "        # re-compose: merge heads with dim_head\n",
    "\n",
    "        out = rearrange(out, \"b h i d -> b i (h d)\")\n",
    "\n",
    "        # Apply final linear transformation layer\n",
    "        return out@self.W_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b1cbb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_dim: token's dimension, i.e. word embedding vector size\n",
    "lha = LinformerAttention(50, heads=8, dim_head=None,\n",
    "                 proj_shape=(100,20), # proj_shape=(seq_len,k_dim)\n",
    "                 trainable_proj=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "332abb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 100, 50])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.uniform(0,1,(1,100,50)) # (batch,seq_len,embed_dim)\n",
    "y = lha(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9709e1",
   "metadata": {},
   "source": [
    "## Linformer Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6caa936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinformerTransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, seq_len, project_dim, num_heads, ff_dim, rate=0.1, trainable_proj=False):\n",
    "        \n",
    "        super(LinformerTransformerBlock, self).__init__()\n",
    "\n",
    "        self.lha = LinformerAttention(embed_dim, heads=num_heads, dim_head=None,\n",
    "                 proj_shape=(seq_len,project_dim), # proj_shape=(seq_len,k_dim)\n",
    "                 trainable_proj=trainable_proj)\n",
    "        \n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.lha(inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12e00ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = LinformerTransformerBlock(50,100, 20, 8, 50, trainable_proj=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf96f193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 100, 50])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.uniform(0,1,(1,100,50)) # (batch,seq_len,embed_dim)\n",
    "y = transformer(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb52e60",
   "metadata": {},
   "source": [
    "## Linear Transformer using Kernel Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd9fa418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100, 10)\n"
     ]
    }
   ],
   "source": [
    "def compute_linear_att(q, k, v):\n",
    "    q = tf.nn.elu(q)+1\n",
    "    k = tf.nn.elu(k)+1\n",
    "    kv = tf.einsum('... s d, ... s m  -> ... m d',k,v)\n",
    "    k_sum = tf.math.reduce_sum(k,axis=1)\n",
    "    z = 1/ (tf.einsum('... l d, ... d -> ... l',q ,k_sum)+1e-4)\n",
    "    Vhat = tf.einsum('... l d, ... m d, ... l -> ... l m',q,kv,z)\n",
    "    return Vhat\n",
    "\n",
    "# x ~ (seq_len,embed_dim) ~ (s,f)\n",
    "# Wq ~ (f,d)\n",
    "# Wk ~ (f,d)\n",
    "# Wv ~ (f,m)\n",
    "# q ~ (s,d)\n",
    "# k ~ (s,d)\n",
    "# v ~ (s,m)\n",
    "\n",
    "s = 100 # seq len\n",
    "f = 10 # embed dim\n",
    "d = 30 # query length\n",
    "m = 10 # key length\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "q = np.ones((BATCH_SIZE,s,d))\n",
    "k = np.ones((BATCH_SIZE,s,d))\n",
    "v = np.ones((BATCH_SIZE,s,m))\n",
    "\n",
    "Vhat = compute_linear_att(q,k,v)\n",
    "print(Vhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "295780a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linear_mhsa(q, k, v):\n",
    "    q = tf.nn.elu(q)+1\n",
    "    k = tf.nn.elu(k)+1\n",
    "    kv = tf.einsum('... h s d, ...  h s m  -> ... h m d',k,v)\n",
    "    k_sum = tf.math.reduce_sum(k,axis=2)\n",
    "    z = 1/ (tf.einsum('... h l d, ... h d -> ... h l',q ,k_sum)+1e-4)\n",
    "    Vhat = tf.einsum('... h l d, ... h m d, ... h l -> ... h l m',q,kv,z)\n",
    "    return Vhat\n",
    "\n",
    "class LinearAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, heads=8):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        self.num_heads = heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention = compute_linear_mhsa(q, k, v)\n",
    "\n",
    "        scaled_attention = tf.transpose(\n",
    "            scaled_attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(\n",
    "            scaled_attention, (batch_size, -1, self.d_model)\n",
    "        )  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class LinearTransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        \n",
    "        super(LinearTransformerBlock, self).__init__()\n",
    "\n",
    "        self.lha = LinearAttention(embed_dim,num_heads)\n",
    "        \n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.lha(inputs,inputs,inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d8f2595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 256)\n",
      "(1, 100, 256)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(0,1,(1,100,256)) # (batch,seq_len,embed_dim)\n",
    "ltb = LinearTransformerBlock(256,2,40)\n",
    "\n",
    "y = ltb(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83efdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f4d1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4febd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3386ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040d4594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2246b31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9578022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac075d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9079258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeb1835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2ff49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be275bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c9850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfp-env",
   "language": "python",
   "name": "anaconda-tfp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
