{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e96e1bfa",
   "metadata": {},
   "source": [
    "# Rosie's Hearing Aid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d20582e",
   "metadata": {},
   "source": [
    "# To-Do\n",
    "* Debug decoder block to check if output dim follows FxCxNc\n",
    "* Make decoder be able to seperate N sources \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95415aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import permutations\n",
    "import evaluation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "import IPython as ip\n",
    "import pywt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0beb33",
   "metadata": {},
   "source": [
    "## Encoder Block\n",
    "![title](Diagrams/encoder.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7397f95",
   "metadata": {},
   "source": [
    "### Chunk Operator \n",
    "Constants: $$ \\lambda \\sim \\text{sample rate} \\newline T \\sim \\text{max duration of inputs in seconds} \\newline C_L \\sim \\text{chunk length}\n",
    "$$\n",
    "Dependants: \n",
    "$$ \\text{N-Chunks} = \\text{floor}({\\frac{\\lambda T}{C_L}}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5455055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkOperator(layers.Layer):\n",
    "    \"\"\" Performs a chunk operation on a 2D (feature,time) tensor \n",
    "    and outputs a 3D (feature,short_time,long_time) tensor. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    samplerate_hz : int\n",
    "        The samlerate of the audio clip \n",
    "    max_input_length_in_seconds : int\n",
    "        The max length of input audio in seconds, if clip is shorter than \n",
    "        max length then zero padding is applied     \n",
    "    chunk_length : int\n",
    "        States length of sliding window in chunk operation over time axis \n",
    "    num_filters_in_encoder: int \n",
    "        Number of filters in Conv1d operation used before chunk operation, this \n",
    "        data is needed for dim/padding problems \n",
    "    batch_size : int \n",
    "        The batch size is needed for reshaping during the chunk operation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.constant \n",
    "        3D (feature,short_time,long_time) tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,samplerate_hz,\n",
    "                 max_input_length_in_seconds,\n",
    "                 chunk_length,\n",
    "                 num_filters_in_encoder,\n",
    "                 batch_size):\n",
    "        super(ChunkOperator,self).__init__()\n",
    "        \n",
    "        # Constants\n",
    "        self.num_filters_in_encoder = num_filters_in_encoder\n",
    "        self.batch_size = batch_size\n",
    "        self.samplerate_hz = samplerate_hz\n",
    "        self.max_input_length_in_seconds = max_input_length_in_seconds\n",
    "        self.chunk_length = chunk_length \n",
    "        \n",
    "        # Dependants\n",
    "        self.num_full_chunks = self.samplerate_hz*self.max_input_length_in_seconds//self.chunk_length\n",
    "        self.signal_length_samples = self.chunk_length*self.num_full_chunks\n",
    "        self.chunk_advance = self.chunk_length // 2\n",
    "        self.num_overlapping_chunks = self.num_full_chunks*2-1\n",
    "        \n",
    "        # The layer itself\n",
    "        self.chunk_operator = tf.keras.layers.Lambda(self.segment_encoded_signal)\n",
    "        \n",
    "    @tf.function\n",
    "    def segment_encoded_signal(self, x):\n",
    "        x1 = tf.reshape(x, (self.batch_size, self.signal_length_samples//self.chunk_length , self.chunk_length , self.num_filters_in_encoder))\n",
    "        x2 = tf.roll(x, shift=-self.chunk_advance, axis=1)\n",
    "        x2 = tf.reshape(x2, (self.batch_size, self.signal_length_samples//self.chunk_length , self.chunk_length , self.num_filters_in_encoder))\n",
    "        x2 = x2[:, :-1, :, :] # Discard last segment with invalid data\n",
    "\n",
    "        x_concat = tf.concat([x1, x2], axis=1)\n",
    "        x = x_concat[:, ::self.num_full_chunks, :, :]\n",
    "        for i in range(1, self.num_full_chunks):\n",
    "            x = tf.concat([x, x_concat[:, i::self.num_full_chunks, :, :]], axis=1)\n",
    "        return x\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.chunk_operator(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94423698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk operator parameters\n",
    "BATCH_SIZE = 1\n",
    "NETWORK_NUM_FILTERS_IN_ENCODER = 64\n",
    "MAX_INPUT_LENGTH_IN_SECONDS = 10\n",
    "SAMPLERATE_HZ = 8000\n",
    "NETWORK_CHUNK_SIZE = 256\n",
    "MAX_INPUT_LENGTH_IN_SECONDS = 10\n",
    "SAMPLERATE_HZ = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "848cbea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk operation test passed!!!\n"
     ]
    }
   ],
   "source": [
    "chunk_layer = ChunkOperator(samplerate_hz=SAMPLERATE_HZ,\n",
    "    max_input_length_in_seconds=MAX_INPUT_LENGTH_IN_SECONDS,\n",
    "    chunk_length=NETWORK_CHUNK_SIZE,\n",
    "    num_filters_in_encoder=NETWORK_NUM_FILTERS_IN_ENCODER,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "x = np.random.uniform(0,1,(1,79872, 64))\n",
    "x_chunk = chunk_layer(x)\n",
    "if x_chunk.shape != (1, 623, 256, 64):\n",
    "    raise ValueError('Chunk operation output is the wrong size!!!')\n",
    "else:\n",
    "    print(\"Chunk operation test passed!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00b1a9e",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6058f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    \"\"\" Takes original audio input x -> ReLU(Conv1D(x)) -> h0\n",
    "         -> LayerNorm(Dense(h0)) -> h1 -> Chunk(h1) -> hc\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_filters_in_encoder: int \n",
    "        Number of filters in Conv1d operation used before chunk operation, this \n",
    "        data is needed for dim/padding problems \n",
    "    encoder_filter_length: int \n",
    "        Length of filter in Conv1D\n",
    "    samplerate_hz : int\n",
    "        The samlerate of the audio clip \n",
    "    max_input_length_in_seconds : int\n",
    "        The max length of input audio in seconds, if clip is shorter than \n",
    "        max length then zero padding is applied     \n",
    "    chunk_length : int\n",
    "        States length of sliding window in chunk operation over time axis \n",
    "    batch_size : int \n",
    "        The batch size is needed for reshaping during the chunk operation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.constant \n",
    "        3D (feature,short_time,long_time) tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,num_filters_in_encoder,\n",
    "                 encoder_filter_length,\n",
    "                 samplerate_hz,\n",
    "                 max_input_length_in_seconds,\n",
    "                 chunk_length,\n",
    "                 batch_size):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        # Constants\n",
    "        self.batch_size = batch_size\n",
    "        self.num_filters_in_encoder = num_filters_in_encoder\n",
    "        self.encoder_filter_length = encoder_filter_length\n",
    "        self.batch_size = batch_size\n",
    "        self.samplerate_hz = samplerate_hz\n",
    "        self.max_input_length_in_seconds = max_input_length_in_seconds\n",
    "        self.chunk_length = chunk_length \n",
    "        \n",
    "        # Dependants\n",
    "        self.encoder_hop_size = self.encoder_filter_length // 2\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.conv1d = tf.keras.layers.Conv1D(filters=self.num_filters_in_encoder, \\\n",
    "                                      kernel_size=self.encoder_filter_length,  \\\n",
    "                                      strides=self.encoder_hop_size, use_bias=False, \\\n",
    "                                      padding=\"same\")\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.chunk_operator = ChunkOperator(samplerate_hz=self.samplerate_hz,\n",
    "                    max_input_length_in_seconds=self.max_input_length_in_seconds,\n",
    "                    chunk_length=self.chunk_length,\n",
    "                    num_filters_in_encoder=self.num_filters_in_encoder,\n",
    "                    batch_size=self.batch_size)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        encoder_out = self.conv1d(tf.expand_dims(inputs, axis=2))\n",
    "        x = self.layer_norm(encoder_out)\n",
    "        x = self.chunk_operator(x)\n",
    "        return x, encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e05a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder block parameters\n",
    "BATCH_SIZE = 1\n",
    "NETWORK_NUM_FILTERS_IN_ENCODER = 64\n",
    "NETWORK_ENCODER_FILTER_LENGTH = 2\n",
    "MAX_INPUT_LENGTH_IN_SECONDS = 10\n",
    "SAMPLERATE_HZ = 8000\n",
    "NETWORK_CHUNK_SIZE = 256\n",
    "MAX_INPUT_LENGTH_IN_SECONDS = 10\n",
    "SAMPLERATE_HZ = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34fa66b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder block test passed!!!\n"
     ]
    }
   ],
   "source": [
    "encoder_block = Encoder(num_filters_in_encoder=NETWORK_NUM_FILTERS_IN_ENCODER,\n",
    "    encoder_filter_length=NETWORK_ENCODER_FILTER_LENGTH,\n",
    "    samplerate_hz=SAMPLERATE_HZ,\n",
    "    max_input_length_in_seconds=MAX_INPUT_LENGTH_IN_SECONDS,\n",
    "    chunk_length=NETWORK_CHUNK_SIZE,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "x = np.random.uniform(0,1,(1,79872))\n",
    "x_chunk, _ = encoder_block(x)\n",
    "if x_chunk.shape != (1, 623, 256, 64):\n",
    "    raise ValueError('Encoder block output is the wrong size!!!')\n",
    "else:\n",
    "    print(\"Encoder block test passed!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9693e17",
   "metadata": {},
   "source": [
    "## Linear Transformer with SineSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ecca019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineSPE(layers.Layer):\n",
    "    def __init__(self, \n",
    "                 num_heads: int = 8,\n",
    "                 in_features: int = 64,\n",
    "                 num_realizations: int = 256,\n",
    "                 num_sines: int = 1):\n",
    "        super(SineSPE, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.in_features = in_features \n",
    "        self.num_sines = num_sines \n",
    "        self.num_realizations = num_realizations\n",
    "        \n",
    "        freqs_init = tf.random_normal_initializer()\n",
    "        self.freqs = tf.Variable(\n",
    "            initial_value=freqs_init(shape=(num_heads, in_features, num_sines), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        offsets_init = tf.random_normal_initializer()\n",
    "        self.offsets = tf.Variable(\n",
    "            initial_value=offsets_init(shape=(num_heads, in_features, num_sines), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        gains_init = tf.random_normal_initializer()\n",
    "        self.gains = tf.Variable(\n",
    "            initial_value=gains_init(shape=(num_heads, in_features, num_sines), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        # Normalize gains \n",
    "        self.gains = self.gains/(tf.math.sqrt(tf.norm(self.gains,axis=-1,keepdims=True))/2)\n",
    "        \n",
    "        # Bias intial freqs\n",
    "        self.freqs = self.freqs-4\n",
    "        \n",
    "        self.code_shape = (num_heads,in_features)\n",
    "\n",
    "    def call(self, shape):\n",
    "        \"\"\"\n",
    "        Generate the code, composed of a random QBar and Kbar,\n",
    "        depending on the parameters, and return them for use with a\n",
    "        SPE module to actually encode queries and keys.\n",
    "        Args:\n",
    "            shape: The outer shape of the inputs: (batchsize, *size)\n",
    "            num_realizations: if provided, overrides self.num_realizations\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(shape) != 2:\n",
    "            raise ValueError('Only 1D inputs are supported by SineSPE')\n",
    "        \n",
    "        max_len = shape[1]\n",
    "        \n",
    "        # build omega_q and omega_k\n",
    "        # with shape (num_heads,keys_dim,length,2*num_sines)\n",
    "        indices = tf.linspace(0,max_len-1,max_len)\n",
    "        indices = tf.cast(indices, dtype=tf.float32)\n",
    "\n",
    "        # make sure freqs are in [0,.5]\n",
    "        freqs = tf.nn.sigmoid(self.freqs[:,:,None,:])/2\n",
    "        \n",
    "        phases_q = 2*math.pi*freqs*indices[None,None,:,None]*self.offsets[:,:,None,:]\n",
    "        omega_q = tf.stack([tf.math.cos(phases_q),tf.math.sin(phases_q)],axis=-1)\n",
    "        omega_q = tf.reshape(omega_q,[1,self.num_heads,self.in_features,max_len,2*self.num_sines] )\n",
    "        \n",
    "        phases_k = 2*math.pi*freqs*indices[None,None,:,None]\n",
    "        omega_k = tf.stack([tf.math.cos(phases_k),tf.math.sin(phases_k)],axis=-1)\n",
    "        omega_k = tf.reshape(omega_k,[1,self.num_heads,self.in_features,max_len,2*self.num_sines] )\n",
    "        \n",
    "        # Gains is (num_heads,keys_dim,num_sines), make nonnegative with softplut\n",
    "        gains = tf.math.softplus(self.gains)\n",
    "        \n",
    "        # Upsample\n",
    "        gains = tf.stack([gains,gains],axis=-1)\n",
    "        gains = tf.reshape(gains, [self.num_heads,self.in_features,2*self.num_sines])\n",
    "        \n",
    "        # Draw noise\n",
    "        z = tf.random.normal((1,self.num_heads,self.in_features,2*self.num_sines,self.num_realizations))\n",
    "        z = z/tf.math.sqrt(tf.cast(self.num_sines*2, dtype=tf.float32))\n",
    "        \n",
    "        # Scale each of the 2*num_sines by the appropriate gain\n",
    "        z = z*gains[None, ..., None]\n",
    "    \n",
    "        # Compute sums over sines\n",
    "        qbar = tf.linalg.matmul(omega_q,z)\n",
    "        kbar = tf.linalg.matmul(omega_k,z)\n",
    "        \n",
    "        # Pemute to (1,length,num_heads,key_dim,num_realization)\n",
    "        qbar = tf.transpose(qbar, perm=[0,3,1,2,4])\n",
    "        kbar = tf.transpose(kbar, perm=[0,3,1,2,4])\n",
    "\n",
    "        # scale\n",
    "        scale = (self.num_realizations*self.in_features)**.25\n",
    "        return (qbar/scale,kbar/scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c34bb009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPEFilter(layers.Layer):\n",
    "    \"\"\"Stochastic positional encoding filter\n",
    "    Applies a positional code provided by a SPE module on actual queries and keys.\n",
    "    Implements gating, i.e. some \"dry\" parameter, that lets original queries and keys through if activated.\n",
    "    Args:\n",
    "    gated: whether to use the gated version, which learns to balance\n",
    "        positional and positionless features.\n",
    "    code_shape: the inner shape of the codes, i.e. (num_heads, key_dim),\n",
    "        as given by `spe.code_shape`\n",
    "    \"\"\"\n",
    "    def __init__(self,gated,code_shape):\n",
    "        super(SPEFilter, self).__init__()\n",
    "\n",
    "        self.gated = gated\n",
    "        self.code_shape = code_shape\n",
    "\n",
    "        # create the gating parameters if required\n",
    "        if gated:\n",
    "            if code_shape is None:\n",
    "                raise RuntimeError('code_shape has to be provided if gated is True.')\n",
    "\n",
    "            gate_init = tf.random_normal_initializer()\n",
    "            self.gate = tf.Variable(\n",
    "                initial_value=gate_init(shape=(code_shape), dtype=\"float32\"),\n",
    "                trainable=True,\n",
    "            )  \n",
    "\n",
    "    def call(self,queries,keys,code):\n",
    "        \"\"\"\n",
    "        Apply SPE on keys with a given code.\n",
    "        Expects keys and queries of shape `(batch_size, ..., num_heads,\n",
    "        key_dim)` and outputs keys and queries of shape `(batch_size,\n",
    "        ..., num_heads, num_realizations)`. code is the tuple\n",
    "        of the 2 tensors provided by the code instance, each one of\n",
    "        shape (1, ..., num_heads, key_dim, num_realizations)\n",
    "        \"\"\"\n",
    "        assert (queries.shape == keys.shape), \\\n",
    "            \"As of current implementation, queries and keys must have the same shape. \"\\\n",
    "            \"got queries: {} and keys: {}\".format(queries.shape, keys.shape)\n",
    "\n",
    "        # qbar and kbar are (1, *shape, num_heads, keys_dim, num_realizations)\n",
    "        (qbar, kbar) = code\n",
    "\n",
    "        # check that codes have the shape we are expecting\n",
    "        if self.code_shape is not None and qbar.shape[-3:-1] != self.code_shape:\n",
    "            raise ValueError(\n",
    "                f'The inner shape of codes is {qbar.shape[-3:-1]}, '\n",
    "                f'but expected {self.code_shape}')\n",
    "\n",
    "        # check shapes: size of codes should be bigger than queries, keys\n",
    "        code_size = qbar.shape[1:-3]\n",
    "        query_size = queries.shape[1:-2]\n",
    "        \n",
    "\n",
    "        #if (len(code_size) != len(query_size)\n",
    "        #    or tf.reduce_any(\n",
    "        #        tf.Variable(code_size) < tf.Variable(query_size)\n",
    "        #    )):\n",
    "        #        raise ValueError(f'Keys/queries have length {query_size}, '\n",
    "        #                         f'but expected at most {code_size}')\n",
    "                \n",
    "        if (len(code_size) != len(query_size)):\n",
    "                raise ValueError(f'Keys/queries have length {query_size}, '\n",
    "                                 f'but expected at most {code_size}')\n",
    "                \n",
    "        if qbar.shape[-3:-1] != queries.shape[-2:]:\n",
    "            raise ValueError(f'shape mismatch. codes have shape {qbar.shape}, '\n",
    "                             f'but queries are {queries.shape}')\n",
    "\n",
    "        # truncate qbar and kbar for matching current queries and keys,\n",
    "        # but only if we need to\n",
    "        for dim in range(len(query_size)):\n",
    "            if code_size[dim] > query_size[dim]:\n",
    "                indices = [slice(1), *[slice(qbar.shape[1+k]) for k in range(dim)],\n",
    "                           slice(query_size[dim])]\n",
    "                qbar = qbar[indices]\n",
    "                kbar = kbar[indices]\n",
    "\n",
    "        # apply gate if required\n",
    "        if self.gated:\n",
    "            # incorporate the constant bias for Pd if required. First draw noise\n",
    "            # such that noise noise^T = 1, for each head, feature, realization.\n",
    "            # qbar is : (1, *shape, num_heads, keys_dim, num_realizations)\n",
    "            in_features = qbar.shape[-2]\n",
    "            num_realizations = qbar.shape[-1]\n",
    "            gating_noise = tf.random.normal(self.code_shape+\\\n",
    "                            (num_realizations,))/(in_features*num_realizations)**.25\n",
    "            \n",
    "            \n",
    "            # normalize it so that it's an additive 1 to Pd\n",
    "            #gating_noise = gating_noise / gating_noise.norm(dim=2, keepdim=True)\n",
    "\n",
    "            # constrain the gate parameter to be in [0 1]\n",
    "            gate = tf.math.sigmoid(self.gate[..., None])\n",
    "\n",
    "            # qbar is (1, *shape, num_heads, keys_dim, num_realizations)\n",
    "            # gating noise is (num_heads, keys_dim, num_realizations)\n",
    "            # gate is (num_heads, keys_dim, 1)\n",
    "            #import ipdb; ipdb.set_trace()\n",
    "            qbar = tf.math.sqrt(1.-gate) * qbar  + tf.math.sqrt(gate) * gating_noise\n",
    "            kbar = tf.math.sqrt(1.-gate) * kbar  + tf.math.sqrt(gate) * gating_noise\n",
    "\n",
    "        # sum over d after multiplying by queries and keys\n",
    "        # qbar/kbar are (1, *shape, num_heads, keys_dim, num_realizations)\n",
    "        # queries/keys  (batchsize, *shape, num_heads, keys_dim)\n",
    "        qhat = tf.math.reduce_sum(qbar * queries[..., None],axis=-2)\n",
    "        khat = tf.math.reduce_sum(kbar * keys[..., None],axis=-2)\n",
    "\n",
    "        # result is (batchsize, ..., num_heads, num_realizations)\n",
    "        return (qhat, khat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaf0fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_linear_mhsa(q, k, v):\n",
    "    q = tf.nn.gelu(q)+1 # Needed for kernel assumption \n",
    "    k = tf.nn.gelu(k)+1\n",
    "    kv = tf.einsum('... h s d, ...  h s m  -> ... h m d',k,v)\n",
    "    k_sum = tf.math.reduce_sum(k,axis=2)\n",
    "    z = 1/ (tf.einsum('... h l d, ... h d -> ... h l',q ,k_sum)+1e-4)\n",
    "    Vhat = tf.einsum('... h l d, ... h m d, ... h l -> ... h l m',q,kv,z)\n",
    "    return Vhat\n",
    "\n",
    "class LinearAttentionSineSPE(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, heads=8, num_sines=5):\n",
    "        super(LinearAttentionSineSPE, self).__init__()\n",
    "        self.num_heads = heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.spe_encoder = SineSPE(num_heads=heads,          # Number of attention heads\n",
    "                          in_features=self.depth,       # Dimension of keys and queries\n",
    "                          num_realizations=self.depth,  # New dimension of keys and queries\n",
    "                          num_sines=num_sines)          # Number of sinusoidal components\n",
    "        self.spe_filter = SPEFilter(gated=True, code_shape=self.spe_encoder.code_shape)\n",
    "\n",
    "    @tf.function\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)   \n",
    "\n",
    "        q = tf.transpose(q,perm=[0,2,1,3])\n",
    "        k = tf.transpose(k,perm=[0,2,1,3])\n",
    "        \n",
    "        pos_codes = self.spe_encoder(q.shape[:2])  # pos_codes is a tuple (qbar, kbar)\n",
    "        q, k = self.spe_filter(q, k, pos_codes)\n",
    "        q = tf.transpose(q,perm=[0,2,1,3])\n",
    "        k = tf.transpose(k,perm=[0,2,1,3])\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention = compute_linear_mhsa(q, k, v)\n",
    "\n",
    "        scaled_attention = tf.transpose(\n",
    "            scaled_attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(\n",
    "            scaled_attention, (batch_size, -1, self.d_model)\n",
    "        )  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eda34bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSineSPETransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        \n",
    "        super(LinearSineSPETransformerBlock, self).__init__()\n",
    "\n",
    "        self.lha = LinearAttentionSineSPE(embed_dim,num_heads)\n",
    "        \n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.lha(inputs,inputs,inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72cc7035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 64)\n",
      "Linear SineSPE Transformer block test passed!!!\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "seq_len = 100\n",
    "n_heads = 8\n",
    "embedding_dim = 64 \n",
    "ff_dim = 40\n",
    "\n",
    "x = np.random.uniform(0,1,(BATCH_SIZE,seq_len,embedding_dim))\n",
    "ltb = LinearSineSPETransformerBlock(embedding_dim, n_heads ,ff_dim)\n",
    "\n",
    "y = ltb(x)\n",
    "\n",
    "print(y.shape)\n",
    "if y.shape != (32, 100, 64):\n",
    "    raise ValueError('Linear SineSPE Transformer block output is the wrong size!!!')\n",
    "else:\n",
    "    print(\"Linear SineSPE Transformer block test passed!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495cc7c9",
   "metadata": {},
   "source": [
    "## Decoder Block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "91102480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \"\"\" Takes Chop Shop output and combines with decoder output \n",
    "        to form sepeated sources.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_sources: int \n",
    "        Number of sources to sepeate audio into\n",
    "    num_filters_in_encoder: int \n",
    "        Number of filters in Conv1d operation used before chunk operation, this \n",
    "        data is needed for dim/padding problems \n",
    "    encoder_filter_length: int \n",
    "        Length of filter in Conv1D\n",
    "    samplerate_hz : int\n",
    "        The samlerate of the audio clip \n",
    "    max_input_length_in_seconds : int\n",
    "        The max length of input audio in seconds, if clip is shorter than \n",
    "        max length then zero padding is applied     \n",
    "    chunk_length : int\n",
    "        States length of sliding window in chunk operation over time axis \n",
    "    batch_size : int \n",
    "        The batch size is needed for reshaping during the chunk operation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.constant \n",
    "        List of seperated signals \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,signal_length_samples,\n",
    "                 n_sources,\n",
    "                 num_filters_in_encoder,\n",
    "                 encoder_filter_length,\n",
    "                 samplerate_hz,\n",
    "                 max_input_length_in_seconds,\n",
    "                 chunk_length,\n",
    "                 batch_size):\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        # Constants\n",
    "        self.n_sources = n_sources\n",
    "        self.batch_size = batch_size\n",
    "        self.num_filters_in_encoder = num_filters_in_encoder\n",
    "        self.encoder_filter_length = encoder_filter_length\n",
    "        self.batch_size = batch_size\n",
    "        self.samplerate_hz = samplerate_hz\n",
    "        self.signal_length_samples = signal_length_samples\n",
    "        self.max_input_length_in_seconds = max_input_length_in_seconds\n",
    "        self.chunk_length = chunk_length \n",
    "        self.chunk_advance =  chunk_length  // 2\n",
    "        \n",
    "        # Dependants\n",
    "        self.encoder_hop_size = self.encoder_filter_length // 2\n",
    "        \n",
    "        self.cut_len = 0\n",
    "        if self.num_filters_in_encoder%self.n_sources != 0:\n",
    "            self.cut_len = self.num_filters_in_encoder%self.n_sources\n",
    "            \n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.overlap_and_add_mask_segments_layer = keras.layers.Lambda(self.overlap_and_add_mask_segments)\n",
    "    \n",
    "        self.DenseLayers = []\n",
    "        for i in range(self.n_sources):\n",
    "            dense_layer = keras.layers.Dense(units=self.encoder_filter_length, use_bias=False,name='OverLapAddDense_'+str(i))\n",
    "            self.DenseLayers.append(dense_layer)\n",
    "            \n",
    "        self.OverLapAndAddDecoderLayers = []\n",
    "        for i in range(self.n_sources):\n",
    "            overlap_and_add_in_decoder_layer = keras.layers.Lambda(self.overlap_and_add_in_decoder,name='OverLapAdd_'+str(i))\n",
    "            self.OverLapAndAddDecoderLayers.append(overlap_and_add_in_decoder_layer)\n",
    "        \n",
    "    @tf.function\n",
    "    def overlap_and_add_mask_segments(self, x):\n",
    "        x = tf.transpose(x, [0, 3, 1, 2])\n",
    "        x = tf.signal.overlap_and_add(x, self.chunk_advance)\n",
    "        return tf.transpose(x, [0, 2, 1])\n",
    "\n",
    "    @tf.function\n",
    "    def overlap_and_add_in_decoder(self, x):\n",
    "        return tf.signal.overlap_and_add(x, self.encoder_hop_size)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x, encoder_out):\n",
    "        masks = self.overlap_and_add_mask_segments_layer(x)\n",
    "        x = masks*encoder_out\n",
    "\n",
    "        if self.cut_len != 0:\n",
    "            x = tf.split(x,num_or_size_splits=[self.num_filters_in_encoder-self.cut_len,self.cut_len], axis=-1, num=None, name='split_cut')[0]\n",
    "        d_sources = tf.split(x, num_or_size_splits=self.n_sources, axis=-1, num=None, name='split')\n",
    "        \n",
    "        decoded_sources = []\n",
    "        for i in range(self.n_sources):\n",
    "            decoded_spk = self.DenseLayers[i](d_sources[i])\n",
    "            decoded_spk = self.OverLapAndAddDecoderLayers[i](decoded_spk)[:, :self.signal_length_samples]\n",
    "            decoded_sources.append(decoded_spk)\n",
    "            \n",
    "        decoded = tf.stack(decoded_sources, axis=1)\n",
    "\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b939a",
   "metadata": {},
   "source": [
    "## Loss Function: Si-SNR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1b561d",
   "metadata": {},
   "source": [
    "Si-SNR $= 10 \\log_{10} \\frac{||x_{target}||^2}{||e_{noise}||^2} $, where \n",
    "$x_{target}=\\frac{< \\hat{x},x > x }{||x||^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "eb948c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@tf.function\\ndef get_permutation_invariant_sisnr(spk0_estimate, spk1_estimate, spk0_groundtruth, spk1_groundtruth):\\n    perm0_spk0 = sisnr(spk0_groundtruth, spk0_estimate, do_expand=True)\\n    perm0_spk1 = sisnr(spk1_groundtruth, spk1_estimate, do_expand=True)\\n    perm1_spk0 = sisnr(spk0_groundtruth, spk1_estimate, do_expand=True)\\n    perm1_spk1 = sisnr(spk1_groundtruth, spk0_estimate, do_expand=True)\\n\\n    # Get best permutation\\n    if perm0_spk0 + perm0_spk1 > perm1_spk0 + perm1_spk1:\\n        return perm0_spk0, perm0_spk1\\n\\n    return perm1_spk0, perm1_spk1 \\n\\n\\n@tf.function\\ndef permutation_invariant_loss(y_true, y_pred,n_sources=2):\\n    # PIT for n-sources, work but very slow, needs to use tf.while_loop\\n    # yet, implementing tf.while_loop is a nightmare\\n    sn, sn_hat = [], []\\n    for i in range(n_sources):\\n        sn.append(y_true[:,i,:])\\n        sn_hat.append(y_pred[:,i,:])\\n        \\n    perm = list(permutations(range(n_sources)))\\n    sisnr_perm = []\\n        \\n    for i in range(len(perm)):\\n        sisnr_perm_spk = [sisnr(sn[p], sn_hat[j]) for p,j in enumerate(perm[i])]\\n        sisnr_perm.append(tf.math.reduce_sum(sisnr_perm_spk)/n_sources)\\n\\n    sisnr_perm = tf.stack(sisnr_perm) \\n    return -tf.math.reduce_max(sisnr_perm)\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "@tf.function\n",
    "def log10(x):\n",
    "    numerator = tf.math.log(x)\n",
    "    denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\n",
    "    return numerator / denominator\n",
    "\n",
    "@tf.function\n",
    "def sisnr(s, s_hat, do_expand=False, eps=1e-8):\n",
    "    if do_expand:\n",
    "        s = np.expand_dims(s, axis=0)\n",
    "        s_hat = np.expand_dims(s_hat, axis=0)\n",
    "    dot_product = tf.math.reduce_sum(s*s_hat, axis=1, keepdims=True)\n",
    "    squares = tf.math.reduce_sum(s*s, axis=1, keepdims=True)\n",
    "    s_target = s * dot_product / squares\n",
    "    e_noise = s_hat - s_target\n",
    "    s_target_squared = tf.math.reduce_sum(s_target*s_target, axis=1)\n",
    "    e_noise_squared = tf.math.reduce_sum(e_noise*e_noise, axis=1)\n",
    "    return 10*log10(s_target_squared / (e_noise_squared + eps))\n",
    "\n",
    "@tf.function\n",
    "def permutation_invariant_loss(y_true, y_pred,n_sources=2):\n",
    "    sn, sn_hat = [], []\n",
    "    for i in range(n_sources):\n",
    "        sn.append(y_true[:,i,:])\n",
    "        sn_hat.append(y_pred[:,i,:])\n",
    "    \n",
    "    if n_sources == 2:\n",
    "        sisnr_perm0_spk0 = sisnr(sn[0], sn_hat[0])\n",
    "        sisnr_perm0_spk1 = sisnr(sn[1], sn_hat[1])\n",
    "        sisnr_perm0 = (sisnr_perm0_spk0 + sisnr_perm0_spk1) / 2\n",
    "\n",
    "        sisnr_perm1_spk0 = sisnr(sn[0], sn_hat[1])\n",
    "        sisnr_perm1_spk1 = sisnr(sn[1], sn_hat[0])\n",
    "        sisnr_perm1 = (sisnr_perm1_spk0 + sisnr_perm1_spk1) / 2\n",
    "\n",
    "        sisnr_perm_invariant = tf.math.maximum(sisnr_perm0, sisnr_perm1)\n",
    "        return -sisnr_perm_invariant\n",
    "    \n",
    "    elif n_sources == 3:\n",
    "        sisnr_perm0_spk0 = sisnr(sn[0], sn_hat[0])\n",
    "        sisnr_perm0_spk1 = sisnr(sn[1], sn_hat[1])\n",
    "        sisnr_perm0_spk2 = sisnr(sn[2], sn_hat[2])\n",
    "        sisnr_perm0 = (sisnr_perm0_spk0 + sisnr_perm0_spk1 + sisnr_perm0_spk2) / 3\n",
    "\n",
    "        sisnr_perm1_spk0 = sisnr(sn[0], sn_hat[0])\n",
    "        sisnr_perm1_spk1 = sisnr(sn[1], sn_hat[2])\n",
    "        sisnr_perm1_spk2 = sisnr(sn[2], sn_hat[1])\n",
    "        sisnr_perm1 = (sisnr_perm1_spk0 + sisnr_perm1_spk1 + sisnr_perm1_spk2) / 3\n",
    "        \n",
    "        sisnr_perm2_spk0 = sisnr(sn[0], sn_hat[1])\n",
    "        sisnr_perm2_spk1 = sisnr(sn[1], sn_hat[0])\n",
    "        sisnr_perm2_spk2 = sisnr(sn[2], sn_hat[2])\n",
    "        sisnr_perm2 = (sisnr_perm2_spk0 + sisnr_perm2_spk1 + sisnr_perm2_spk2) / 3\n",
    "        \n",
    "        sisnr_perm3_spk0 = sisnr(sn[0], sn_hat[1])\n",
    "        sisnr_perm3_spk1 = sisnr(sn[1], sn_hat[2])\n",
    "        sisnr_perm3_spk2 = sisnr(sn[2], sn_hat[0])\n",
    "        sisnr_perm3 = (sisnr_perm3_spk0 + sisnr_perm3_spk1 + sisnr_perm3_spk2) / 3\n",
    "        \n",
    "        sisnr_perm4_spk0 = sisnr(sn[0], sn_hat[2])\n",
    "        sisnr_perm4_spk1 = sisnr(sn[1], sn_hat[0])\n",
    "        sisnr_perm4_spk2 = sisnr(sn[2], sn_hat[1])\n",
    "        sisnr_perm4 = (sisnr_perm4_spk0 + sisnr_perm4_spk1 + sisnr_perm4_spk2) / 3\n",
    "        \n",
    "        sisnr_perm5_spk0 = sisnr(sn[0], sn_hat[2])\n",
    "        sisnr_perm5_spk1 = sisnr(sn[1], sn_hat[1])\n",
    "        sisnr_perm5_spk2 = sisnr(sn[2], sn_hat[0])\n",
    "        sisnr_perm5 = (sisnr_perm5_spk0 + sisnr_perm5_spk1 + sisnr_perm5_spk2) / 3\n",
    "        \n",
    "        sisnr_perm_invariant = tf.stack([sisnr_perm1,sisnr_perm2,sisnr_perm3,sisnr_perm4,sisnr_perm5])\n",
    "        sisnr_perm_invariant = tf.math.reduce_max(sisnr_perm_invariant)\n",
    "        return -sisnr_perm_invariant\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "@tf.function\n",
    "def get_permutation_invariant_sisnr(spk0_estimate, spk1_estimate, spk0_groundtruth, spk1_groundtruth):\n",
    "    perm0_spk0 = sisnr(spk0_groundtruth, spk0_estimate, do_expand=True)\n",
    "    perm0_spk1 = sisnr(spk1_groundtruth, spk1_estimate, do_expand=True)\n",
    "    perm1_spk0 = sisnr(spk0_groundtruth, spk1_estimate, do_expand=True)\n",
    "    perm1_spk1 = sisnr(spk1_groundtruth, spk0_estimate, do_expand=True)\n",
    "\n",
    "    # Get best permutation\n",
    "    if perm0_spk0 + perm0_spk1 > perm1_spk0 + perm1_spk1:\n",
    "        return perm0_spk0, perm0_spk1\n",
    "\n",
    "    return perm1_spk0, perm1_spk1 \n",
    "\n",
    "\n",
    "@tf.function\n",
    "def permutation_invariant_loss(y_true, y_pred,n_sources=2):\n",
    "    # PIT for n-sources, work but very slow, needs to use tf.while_loop\n",
    "    # yet, implementing tf.while_loop is a nightmare\n",
    "    sn, sn_hat = [], []\n",
    "    for i in range(n_sources):\n",
    "        sn.append(y_true[:,i,:])\n",
    "        sn_hat.append(y_pred[:,i,:])\n",
    "        \n",
    "    perm = list(permutations(range(n_sources)))\n",
    "    sisnr_perm = []\n",
    "        \n",
    "    for i in range(len(perm)):\n",
    "        sisnr_perm_spk = [sisnr(sn[p], sn_hat[j]) for p,j in enumerate(perm[i])]\n",
    "        sisnr_perm.append(tf.math.reduce_sum(sisnr_perm_spk)/n_sources)\n",
    "\n",
    "    sisnr_perm = tf.stack(sisnr_perm) \n",
    "    return -tf.math.reduce_max(sisnr_perm)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "426d4c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pit_loss(y_true, y_pred, loss_type, batch_size, n_speaker, n_output, pit_axis=1):\n",
    "    # [batch, spk #, length]\n",
    "    real_spk_num = n_speaker\n",
    "\n",
    "    # TODO 1: # output channel != # speaker\n",
    "    v_perms = tf.constant(list(permutations(range(n_output), n_speaker)))\n",
    "    v_perms_onehot = tf.one_hot(v_perms, n_output)\n",
    "\n",
    "    y_true_exp = tf.expand_dims(y_true, pit_axis+1) # [batch, n_speaker, 1,        len]\n",
    "    y_pred_exp = tf.expand_dims(y_pred, pit_axis)   # [batch, 1,         n_output, len]\n",
    "\n",
    "    cross_total_loss = get_loss(loss_type, y_true_exp, y_pred_exp)\n",
    "\n",
    "    loss_sets = tf.einsum('bij,pij->bp', cross_total_loss, v_perms_onehot) \n",
    "    loss = tf.reduce_min(loss_sets, axis=1)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "        \n",
    "    # find permutation sets for y pred\n",
    "    s_perm_sets = tf.argmin(loss_sets, 1)\n",
    "    s_perm_choose = tf.gather(v_perms, s_perm_sets)\n",
    "    s_perm_idxs = tf.stack([\n",
    "        tf.tile(\n",
    "            tf.expand_dims(tf.range(batch_size), 1),\n",
    "            [1, n_speaker]),\n",
    "        s_perm_choose], axis=2)\n",
    "\n",
    "    s_perm_idxs = tf.reshape(s_perm_idxs, [batch_size*n_speaker, 2])\n",
    "    y_pred = tf.gather_nd(y_pred, s_perm_idxs)\n",
    "    y_pred = tf.reshape(y_pred, [batch_size, n_speaker, -1])\n",
    "\n",
    "    if loss_type != 'sdr':\n",
    "        sdr = evaluation.sdr(y_true[:,:real_spk_num,:], y_pred[:,:real_spk_num,:])\n",
    "        sdr = tf.reduce_mean(sdr)\n",
    "    else:\n",
    "        sdr = -loss/n_speaker\n",
    "\n",
    "    return loss, y_pred, sdr, s_perm_choose\n",
    "\n",
    "def get_loss(loss_type, t_true_exp, t_pred_exp, axis=-1):\n",
    "    if loss_type == 'l1':\n",
    "        y_cross_loss = t_true_exp - t_pred_exp\n",
    "        cross_total_loss = tf.reduce_sum(tf.abs(y_cross_loss), axis=axis)\n",
    "\n",
    "    elif loss_type == 'l2':\n",
    "        y_cross_loss = t_true_exp - t_pred_exp\n",
    "        cross_total_loss = tf.reduce_sum(tf.square(y_cross_loss), axis=axis)\n",
    "\n",
    "    elif loss_type == 'snr':\n",
    "        cross_total_loss = -evaluation.snr(t_true_exp, t_pred_exp)\n",
    "\n",
    "    elif loss_type == 'sdr':\n",
    "        cross_total_loss = -evaluation.sdr(t_true_exp, t_pred_exp)\n",
    "\n",
    "    elif loss_type == 'sisnr':\n",
    "        cross_total_loss = -evaluation.sisnr(t_true_exp, t_pred_exp)\n",
    "\n",
    "    elif loss_type == 'sdr_modify':\n",
    "        cross_total_loss = -evaluation.sdr_modify(t_true_exp, t_pred_exp)\n",
    "\n",
    "    elif loss_type == 'sisdr':\n",
    "        cross_total_loss = -evaluation.sisdr(t_true_exp, t_pred_exp)\n",
    "\n",
    "    elif loss_type == 'sym_sisdr':\n",
    "        cross_total_loss = -evaluation.sym_sisdr(t_true_exp, t_pred_exp)\n",
    "\n",
    "    return cross_total_loss\n",
    "\n",
    "@tf.function\n",
    "def sisnri_sdri(s, s_est, mix_s, batch_size, n_speaker, n_output, pit_axis=1):\n",
    "    mix_s = tf.repeat(mix_s,n_speaker,axis=1) \n",
    "    mix_s = tf.reshape(mix_s,(batch_size,n_speaker,-1))\n",
    "    \n",
    "    loss, _, sdr, _ = pit_loss(s, s_est,  'sisnr', batch_size, n_speaker, n_output, pit_axis=1)\n",
    "    loss_b, _, sdr_b, _ = pit_loss(s, mix_s, 'sisnr', batch_size, n_speaker, n_output, pit_axis=1)\n",
    "    \n",
    "    loss *= -1\n",
    "    loss_b *= -1\n",
    "    \n",
    "    if tf.math.is_nan(loss_b) and tf.math.is_nan(sdr_b) == False:\n",
    "        return loss*-1, sdr-sdr_b\n",
    "    elif tf.math.is_nan(loss_b) == False and tf.math.is_nan(sdr_b):\n",
    "        return loss-loss_b, sdr_b\n",
    "    elif tf.math.is_nan(loss_b) and tf.math.is_nan(sdr_b):\n",
    "        return loss*-1, sdr\n",
    "    else:\n",
    "        return loss-loss_b, sdr-sdr_b\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72506330",
   "metadata": {},
   "source": [
    "## RHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "50fe93a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RHA(keras.Model):\n",
    "    def __init__(self,batch_size,\n",
    "                model_weights_file,\n",
    "                num_filters_in_encoder,\n",
    "                encoder_filter_length,\n",
    "                chunk_size,\n",
    "                num_full_chunks,\n",
    "                num_chop_blocks,\n",
    "                num_tran_blocks,\n",
    "                num_head_per_att,\n",
    "                dim_key_att,\n",
    "                max_input_length_in_seconds,\n",
    "                samplerate_hz,\n",
    "                num_speakers):\n",
    "        super(RHA, self).__init__()\n",
    "        \n",
    "        if num_speakers <= 1 or isinstance(num_speakers , int) != True:\n",
    "            raise AssertionError('Passed value for num_speakrs is invalid, must be int greater than one.')\n",
    "        self.num_speakers = num_speakers\n",
    "        \n",
    "        self.num_chop_blocks = num_chop_blocks \n",
    "        self.batch_size = batch_size\n",
    "        self.model_weights_file = model_weights_file\n",
    "        self.max_input_length_in_seconds = max_input_length_in_seconds\n",
    "        self.num_tran_blocks = num_tran_blocks\n",
    "        self.num_head_per_att = num_head_per_att\n",
    "        self.dim_key_att = dim_key_att\n",
    "        self.encoder_filter_length = encoder_filter_length\n",
    "        self.num_filters_in_encoder = num_filters_in_encoder\n",
    "        self.encoder_hop_size = encoder_filter_length // 2\n",
    "        self.num_full_chunks = num_full_chunks\n",
    "        self.signal_length_samples = chunk_size*num_full_chunks\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_advance = chunk_size // 2\n",
    "        self.num_overlapping_chunks = num_full_chunks*2-1\n",
    "        self.samplerate_hz = samplerate_hz\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self.getModel()\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss_sisnr\")\n",
    "        self.sdr_tracker = keras.metrics.Mean(name=\"sdr\")\n",
    "        self.sdri_tracker = keras.metrics.Mean(name=\"sdri\")\n",
    "        self.sisnri_tracker = keras.metrics.Mean(name=\"sisnri\")\n",
    "        \n",
    "    def getModel(self):\n",
    "        # Model input \n",
    "        inputs = tf.keras.Input(self.signal_length_samples)\n",
    "        \n",
    "        # Encoder Block \n",
    "        z, encoder_out = Encoder(num_filters_in_encoder=self.num_filters_in_encoder,\n",
    "                    encoder_filter_length=self.encoder_filter_length,\n",
    "                    samplerate_hz=self.samplerate_hz,\n",
    "                    max_input_length_in_seconds=self.max_input_length_in_seconds,\n",
    "                    chunk_length=self.chunk_size,\n",
    "                    batch_size=self.batch_size)(inputs)\n",
    "        \n",
    "        # Chop Shop \n",
    "        for i in range(self.num_chop_blocks):  \n",
    "            # Intra ~ Short\n",
    "            z = tf.reshape(z,(self.batch_size,-1,self.chunk_size))\n",
    "            _ , _ , dim_check = z.get_shape()\n",
    "            if dim_check % self.num_head_per_att != 0:\n",
    "                for i in range(self.num_tran_blocks):\n",
    "                    if i > 0: \n",
    "                        z = tf.reshape(z,(self.batch_size,-1,self.chunk_size))\n",
    "                    pad_len = self.num_head_per_att-(dim_check % self.num_head_per_att)\n",
    "                    zero_pad = tf.zeros((self.batch_size,self.num_overlapping_chunks*self.num_filters_in_encoder,pad_len))\n",
    "                    z_pad = tf.concat([z,zero_pad], axis=-1)\n",
    "                    x = LinearSineSPETransformerBlock(self.chunk_size+pad_len,self.num_head_per_att,self.dim_key_att)(z_pad)\n",
    "                    x = x[:,:,:-pad_len]\n",
    "                    z = x+z\n",
    "                    z = tf.reshape(z,(self.batch_size,self.num_overlapping_chunks,self.chunk_size,self.num_filters_in_encoder))\n",
    "            else:      \n",
    "                for i in range(self.num_tran_blocks):\n",
    "                    if i > 0: \n",
    "                        z = tf.reshape(z,(self.batch_size,-1,self.chunk_size))\n",
    "                    x = LinearSineSPETransformerBlock(self.chunk_size,self.num_head_per_att,self.dim_key_att)(z)\n",
    "                    z = x+z\n",
    "                    z = tf.reshape(z,(self.batch_size,self.num_overlapping_chunks,self.chunk_size,self.num_filters_in_encoder))\n",
    "\n",
    "            # Inter ~ Long \n",
    "            z = tf.reshape(z,(self.batch_size,-1,self.num_overlapping_chunks))\n",
    "            _ , _ , dim_check = z.get_shape()\n",
    "            if dim_check % self.num_head_per_att != 0:\n",
    "                # Case 1: padding needed\n",
    "                for i in range(self.num_tran_blocks):\n",
    "                    if i > 0:  \n",
    "                        z = tf.reshape(z,(self.batch_size,-1,self.num_overlapping_chunks))\n",
    "                    pad_len = self.num_head_per_att-(dim_check % self.num_head_per_att)\n",
    "                    zero_pad = tf.zeros((self.batch_size,self.chunk_size*self.num_filters_in_encoder,pad_len))\n",
    "                    z_pad = tf.concat([z,zero_pad], axis=-1)\n",
    "                    x = LinearSineSPETransformerBlock(self.num_overlapping_chunks+pad_len,self.num_head_per_att,self.dim_key_att)(z_pad)\n",
    "                    x = x[:,: ,:-pad_len]\n",
    "                    z = x+z\n",
    "                z = tf.reshape(z,(self.batch_size,self.num_overlapping_chunks,self.chunk_size,self.num_filters_in_encoder))\n",
    "            else:    \n",
    "                # Case 2: padding not needed\n",
    "                for i in range(self.num_tran_blocks):\n",
    "                    if i > 0: \n",
    "                        z = tf.reshape(z,(self.batch_size,-1,self.chunk_size))\n",
    "                    x = LinearSineSPETransformerBlock(self.num_overlapping_chunks,self.num_head_per_att,self.dim_key_att)(z)\n",
    "                    z = x+z\n",
    "                    z = tf.reshape(z,(self.batch_size,self.num_overlapping_chunks,self.chunk_size,self.num_filters_in_encoder))\n",
    "        x = z\n",
    "        \n",
    "        # Decoder Block \n",
    "        decoded = Decoder(signal_length_samples=self.signal_length_samples,\n",
    "                    n_sources=self.num_speakers,\n",
    "                    num_filters_in_encoder=self.num_filters_in_encoder,\n",
    "                    encoder_filter_length=self.encoder_filter_length,\n",
    "                    samplerate_hz=self.samplerate_hz,\n",
    "                    max_input_length_in_seconds=self.max_input_length_in_seconds,\n",
    "                    chunk_length=self.chunk_size,\n",
    "                    batch_size=self.batch_size)(x,encoder_out)\n",
    "        \n",
    "        # Final model \n",
    "        model = tf.keras.Model(inputs, decoded)\n",
    "        return model \n",
    "        \n",
    "    def call(self,inputs):\n",
    "        yh = self.model(inputs)\n",
    "        return yh\n",
    "        \n",
    "    def train_step(self, inputs):\n",
    "        X, y = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            yh = self.model(X)\n",
    "            #loss = permutation_invariant_loss(y,yh,n_sources=self.num_speakers)\n",
    "            loss, y_pred, sdr, s_perm_choose = pit_loss(y, yh, 'sisnr', self.batch_size, self.num_speakers, self.num_speakers, pit_axis=1)\n",
    "            \n",
    "        y_mix = tf.math.reduce_sum(y,axis=1,keepdims=True)\n",
    "        sisnri_val, sdri_val = sisnri_sdri(y, yh, y_mix,  self.batch_size, self.num_speakers, self.num_speakers, pit_axis=1)\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.sdr_tracker.update_state(sdr)\n",
    "        self.sdri_tracker.update_state(sdri_val)\n",
    "        self.sisnri_tracker.update_state(sisnri_val)\n",
    "        return {\n",
    "            \"si-snr\": self.loss_tracker.result()*-1,\n",
    "            \"sdr\": self.sdr_tracker.result(),\n",
    "            \"si-snri\": self.sisnri_tracker.result(),\n",
    "            \"sdri\": self.sdri_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        \"\"\"List of the model's metrics.\n",
    "        We make sure the loss tracker is listed as part of `model.metrics`\n",
    "        so that `fit()` and `evaluate()` are able to `reset()` the loss tracker\n",
    "        at the start of each epoch and at the start of an `evaluate()` call.\n",
    "        \"\"\"\n",
    "        return [self.loss_tracker,self.sdr_tracker,self.sisnri_tracker,self.sdri_tracker]\n",
    "\n",
    "    def test_step(self, inputs):\n",
    "        X,y = inputs\n",
    "        yh = self.model(X)\n",
    "        #loss = permutation_invariant_loss(y,yh,n_sources=self.num_speakers)\n",
    "        loss, y_pred, sdr, s_perm_choose = pit_loss(y, yh, 'sisnr', self.batch_size, self.num_speakers, self.num_speakers, pit_axis=1)\n",
    "        y_mix = tf.math.reduce_sum(y,axis=1,keepdims=True)\n",
    "        sisnri_val, sdri_val = sisnri_sdri(y, yh, y_mix,  self.batch_size, self.num_speakers, self.num_speakers, pit_axis=1)\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.sdr_tracker.update_state(sdr)\n",
    "        self.sdri_tracker.update_state(sdri_val)\n",
    "        self.sisnri_tracker.update_state(sisnri_val)\n",
    "        return {\n",
    "            \"si-snr\": self.loss_tracker.result()*-1,\n",
    "            \"sdr\": self.sdr_tracker.result(),\n",
    "            \"si-snri\": self.sisnri_tracker.result(),\n",
    "            \"sdri\": self.sdri_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "de224a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "OPTIMIZER_CLIP_L2_NORM_VALUE = 5\n",
    "\n",
    "NETWORK_NUM_SPEAKERS = 4\n",
    "MAX_INPUT_LENGTH_IN_SECONDS = 6\n",
    "SAMPLERATE_HZ = 8000\n",
    "\n",
    "NETWORK_NUM_FILTERS_IN_ENCODER = 64\n",
    "NETWORK_ENCODER_FILTER_LENGTH = 2\n",
    "NETWORK_NUM_HEAD_PER_ATT = 8\n",
    "NEWORK_DIM_KEY_ATT = 1024\n",
    "NETWORK_NUM_TRAN_BLOCKS = 1\n",
    "NETWORK_NUM_CHOP_BLOCKS = 1\n",
    "NETWORK_CHUNK_SIZE = 256\n",
    "\n",
    "NUM_CHUNKS = SAMPLERATE_HZ*MAX_INPUT_LENGTH_IN_SECONDS//NETWORK_CHUNK_SIZE\n",
    "num_overlapping_chunks = NUM_CHUNKS*2-1\n",
    "\n",
    "print(num_overlapping_chunks)\n",
    "print((num_overlapping_chunks*NETWORK_CHUNK_SIZE)%NETWORK_NUM_SPEAKERS)\n",
    "\n",
    "# (1, 373, 256, 64) = (batch_size, num_overlapping_chunks, \\\n",
    "#            NETWORK_CHUNK_SIZE, NETWORK_NUM_FILTERS_IN_ENCODER)\n",
    "# 47872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dfc39719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_33 (InputLayer)           [(None, 47872)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_33 (Encoder)            ((1, 373, 256, 64),  256         input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_128 (TFOpLambda)     (1, 23872, 256)      0           encoder_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "linear_sine_spe_transformer_blo (1, 23872, 256)      791296      tf.reshape_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_64 (TFOpLa (1, 23872, 256)      0           linear_sine_spe_transformer_block\n",
      "                                                                 tf.reshape_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_129 (TFOpLambda)     (1, 373, 256, 64)    0           tf.__operators__.add_64[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_130 (TFOpLambda)     (1, 16384, 373)      0           tf.reshape_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat_32 (TFOpLambda)       (1, 16384, 376)      0           tf.reshape_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "linear_sine_spe_transformer_blo (1, 16384, 376)      1342216     tf.concat_32[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_32 (Sl (1, 16384, 373)      0           linear_sine_spe_transformer_block\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_65 (TFOpLa (1, 16384, 373)      0           tf.__operators__.getitem_32[0][0]\n",
      "                                                                 tf.reshape_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape_131 (TFOpLambda)     (1, 373, 256, 64)    0           tf.__operators__.add_65[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_32 (Decoder)            (None, 4, None)      128         tf.reshape_131[0][0]             \n",
      "                                                                 encoder_33[0][1]                 \n",
      "==================================================================================================\n",
      "Total params: 2,133,896\n",
      "Trainable params: 2,133,896\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model =  RHA(batch_size=BATCH_SIZE,\n",
    "                model_weights_file=None,\n",
    "                num_filters_in_encoder=NETWORK_NUM_FILTERS_IN_ENCODER,\n",
    "                encoder_filter_length=NETWORK_ENCODER_FILTER_LENGTH,\n",
    "                chunk_size=NETWORK_CHUNK_SIZE,\n",
    "                num_full_chunks=NUM_CHUNKS,\n",
    "                num_chop_blocks=NETWORK_NUM_CHOP_BLOCKS,\n",
    "                num_tran_blocks=NETWORK_NUM_TRAN_BLOCKS,\n",
    "                num_head_per_att=NETWORK_NUM_HEAD_PER_ATT,\n",
    "                dim_key_att=NEWORK_DIM_KEY_ATT,\n",
    "                max_input_length_in_seconds=MAX_INPUT_LENGTH_IN_SECONDS,\n",
    "                samplerate_hz=SAMPLERATE_HZ,\n",
    "                num_speakers=NETWORK_NUM_SPEAKERS)\n",
    "\n",
    "opt = AdamW(1e-4,clipnorm=OPTIMIZER_CLIP_L2_NORM_VALUE)\n",
    "model.compile(optimizer=opt,metrics=[\"mse\"],)\n",
    "\n",
    "print(model.model.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74cfe4",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_along_axis(array: np.ndarray, target_length: int, axis: int = 0):\n",
    "\n",
    "    pad_size = target_length - array.shape[axis]\n",
    "\n",
    "    if pad_size <= 0:\n",
    "        return array\n",
    "\n",
    "    npad = [(0, 0)] * array.ndim\n",
    "    npad[axis] = (0, pad_size)\n",
    "\n",
    "    return np.pad(array, pad_width=npad, mode='constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c0a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_data, music_fs = librosa.load('street_music_sample.wav', sr=SAMPLERATE_HZ)\n",
    "dog_data, dog_fs = librosa.load('dog_sample.wav', sr=SAMPLERATE_HZ)\n",
    "print(len(music_data),music_fs,librosa.get_duration(y=music_data, sr=music_fs))\n",
    "print(len(dog_data),dog_fs,librosa.get_duration(y=dog_data, sr=dog_fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf00239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase dog bark\n",
    "dog_data = dog_data*2\n",
    "\n",
    "# Pad audio\n",
    "music_data = pad_along_axis(music_data,47872)\n",
    "dog_data = pad_along_axis(dog_data,47872)\n",
    "print(len(music_data),music_fs,librosa.get_duration(y=music_data, sr=music_fs))\n",
    "print(len(dog_data),dog_fs,librosa.get_duration(y=dog_data, sr=dog_fs))\n",
    "\n",
    "# Mix audio \n",
    "mix_data = music_data+dog_data\n",
    "mix_data = mix_data[np.newaxis,...]\n",
    "print(mix_data.shape)\n",
    "\n",
    "# Stack audio for target \n",
    "target = np.stack([dog_data,music_data])\n",
    "target = target[np.newaxis,...]\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da732d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(music_data, music_fs)\n",
    "status = sd.wait() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a27247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(music_data.shape[0]),music_data)\n",
    "plt.title('Music Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c3661",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(dog_data, dog_fs)\n",
    "status = sd.wait() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12de4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(dog_data.shape[0]),dog_data)\n",
    "plt.title('Dog Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d5781",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(mix_data[0,:], dog_fs)\n",
    "status = sd.wait() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743844b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(mix_data[0,:].shape[0]),mix_data[0,:])\n",
    "plt.title('Mix Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887b986",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df753107",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.load_weights('test_weights.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a2ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(mix_data, target, epochs=1, batch_size=1)\n",
    "#model.model.save_weights('test_weights.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e91b2",
   "metadata": {},
   "source": [
    "## Test model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yh = model(mix_data).numpy()\n",
    "y1, y2 = yh[0,0,:], yh[0,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6487ba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(y1, SAMPLERATE_HZ)\n",
    "status = sd.wait() \n",
    "sf.write('seperated_outputA.wav', y1, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac579d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(y2, SAMPLERATE_HZ)\n",
    "status = sd.wait() \n",
    "sf.write('seperated_outputB.wav', y2, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(mix_data[0,:], dog_fs)\n",
    "status = sd.wait() \n",
    "sf.write('original_mix_input.wav', mix_data[0,:], 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd82b3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952c0969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d9da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfp-env",
   "language": "python",
   "name": "anaconda-tfp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
